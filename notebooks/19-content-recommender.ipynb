{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations via Dimensionality Reduction\n",
    "\n",
    "All the content discovery approaches we have explored in previous notebooks can be used to do content recommendations. Here we explore yet another approach to do that, but instead of considering a single article as input, we will look at situations where we know that a user has read a set of articles and he is looking for recommendations on what to read next.\n",
    "\n",
    "Since we have already extracted the authors, orgs and keywords for each article, we can now construct a bipartite graph between author and article, orgs and article and keywords and article, which gives us the basis for a recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "SOLR_URL = \"http://localhost:8983/solr/nips2index\"\n",
    "FEATURES_DUMP_FILE = os.path.join(DATA_DIR, \"comb-features.tsv\")\n",
    "\n",
    "NMF_MODEL_FILE = os.path.join(MODEL_DIR, \"recommender-nmf.pkl\")\n",
    "\n",
    "PAPERS_METADATA = os.path.join(DATA_DIR, \"papers_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 documents (0 keywords, 0 authors, 0 orgs) retrieved\n",
      "1000 documents (1628 keywords, 1347 authors, 159 orgs) retrieved\n",
      "2000 documents (1756 keywords, 2601 authors, 214 orgs) retrieved\n",
      "3000 documents (1814 keywords, 3948 authors, 269 orgs) retrieved\n",
      "4000 documents (1833 keywords, 5210 authors, 311 orgs) retrieved\n",
      "5000 documents (1842 keywords, 6537 authors, 350 orgs) retrieved\n",
      "6000 documents (1847 keywords, 7983 authors, 385 orgs) retrieved\n",
      "7000 documents (1847 keywords, 9517 authors, 420 orgs) retrieved\n",
      "7238 documents (1847 keywords, 9719 authors, 426 orgs) retrieved, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "query_string = \"*:*\"\n",
    "field_list = \"id,keywords,authors,orgs\"\n",
    "cursor_mark = \"*\"\n",
    "num_docs, num_keywords = 0, 0\n",
    "doc_keyword_pairs = []\n",
    "\n",
    "fdump = open(FEATURES_DUMP_FILE, \"w\")\n",
    "all_keywords, all_authors, all_orgs = set(), set(), set()\n",
    "\n",
    "while True:\n",
    "    if num_docs % 1000 == 0:\n",
    "        print(\"{:d} documents ({:d} keywords, {:d} authors, {:d} orgs) retrieved\"\n",
    "              .format(num_docs, len(all_keywords), len(all_authors), len(all_orgs)))\n",
    "    payload = {\n",
    "        \"q\": query_string,\n",
    "        \"fl\": field_list,\n",
    "        \"sort\": \"id asc\",\n",
    "        \"rows\": 100,\n",
    "        \"cursorMark\": cursor_mark\n",
    "    }\n",
    "    params = urllib.parse.urlencode(payload, quote_via=urllib.parse.quote_plus)\n",
    "    search_url = SOLR_URL + \"/select?\" + params\n",
    "    resp = requests.get(search_url)\n",
    "    resp_json = json.loads(resp.text)\n",
    "    docs = resp_json[\"response\"][\"docs\"]\n",
    "    \n",
    "    docs_retrieved = 0\n",
    "    for doc in docs:\n",
    "        doc_id = int(doc[\"id\"])\n",
    "        keywords, authors, orgs = [\"NA\"], [\"NA\"], [\"NA\"]\n",
    "        if \"keywords\" in doc.keys():\n",
    "            keywords = doc[\"keywords\"]\n",
    "            all_keywords.update(keywords)\n",
    "        if \"authors\" in doc.keys():\n",
    "            authors = doc[\"authors\"]\n",
    "            all_authors.update(authors)\n",
    "        if \"orgs\" in doc.keys():\n",
    "            orgs = doc[\"orgs\"]\n",
    "            all_orgs.update(orgs)\n",
    "        fdump.write(\"{:d}\\t{:s}\\t{:s}\\t{:s}\\n\"\n",
    "                    .format(doc_id, \"|\".join(keywords), \"|\".join(authors), \n",
    "                            \"|\".join(orgs)))\n",
    "        num_docs += 1\n",
    "        docs_retrieved += 1\n",
    "    if docs_retrieved == 0:\n",
    "        break\n",
    "\n",
    "    # for next batch of ${rows} rows\n",
    "    cursor_mark = resp_json[\"nextCursorMark\"]\n",
    "\n",
    "print(\"{:d} documents ({:d} keywords, {:d} authors, {:d} orgs) retrieved, COMPLETE\"\n",
    "      .format(num_docs, len(all_keywords), len(all_authors), len(all_orgs)))\n",
    "fdump.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sparse feature vector for documents\n",
    "\n",
    "The feature vector for each document will consist of a sparse vector of size 11992 (1847+9719+426). An entry is 1 if the item occurs in the document, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1847 9719 426\n"
     ]
    }
   ],
   "source": [
    "def build_lookup_table(item_set):\n",
    "    item2idx = {}\n",
    "    for idx, item in enumerate(item_set):\n",
    "        item2idx[item] = idx\n",
    "    return item2idx\n",
    "\n",
    "keyword2idx = build_lookup_table(all_keywords)\n",
    "author2idx = build_lookup_table(all_authors)\n",
    "org2idx = build_lookup_table(all_orgs)\n",
    "print(len(keyword2idx), len(author2idx), len(org2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7238, 11992)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def build_feature_vector(items, item2idx):\n",
    "    vec = np.zeros((len(item2idx)))\n",
    "    if items == \"NA\":\n",
    "        return vec\n",
    "    for item in items.split(\"|\"):\n",
    "        idx = item2idx[item]\n",
    "        vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "Xk = np.zeros((num_docs, len(keyword2idx)))\n",
    "Xa = np.zeros((num_docs, len(author2idx)))\n",
    "Xo = np.zeros((num_docs, len(org2idx)))\n",
    "\n",
    "fdump = open(FEATURES_DUMP_FILE, \"r\")\n",
    "for line in fdump:\n",
    "    doc_id, keywords, authors, orgs = line.strip().split(\"\\t\")\n",
    "    doc_id = int(doc_id)\n",
    "    Xk[doc_id] = build_feature_vector(keywords, keyword2idx)\n",
    "    Xa[doc_id] = build_feature_vector(authors, author2idx)\n",
    "    Xo[doc_id] = build_feature_vector(orgs, org2idx)\n",
    "fdump.close()    \n",
    "\n",
    "X = np.concatenate((Xk, Xa, Xo), axis=1)\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce dimensionality\n",
    "\n",
    "We reduce the sparse feature vector to a lower dimensional dense vector which effectively maps the original vector to a new \"taste\" vector space. Topic modeling has the same effect. We will use non-negative matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already generated, loading\n",
      "violation: 1.0\n",
      "violation: 0.2411207712867099\n",
      "violation: 0.0225518954481444\n",
      "violation: 0.00395945567371017\n",
      "violation: 0.0004979448419219516\n",
      "violation: 8.176770536033433e-05\n",
      "Converged at iteration 6\n",
      "(7238, 150) (150, 11992)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(NMF_MODEL_FILE):\n",
    "    print(\"model already generated, loading\")\n",
    "    model = joblib.load(NMF_MODEL_FILE)\n",
    "    W = model.transform(X)\n",
    "    H = model.components_\n",
    "else:    \n",
    "    model = NMF(n_components=150, init='random', solver=\"cd\", \n",
    "                verbose=True, random_state=42)\n",
    "    W = model.fit_transform(X)\n",
    "    H = model.components_\n",
    "    joblib.dump(model, NMF_MODEL_FILE)\n",
    "    \n",
    "print(W.shape, H.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7238, 7238)\n"
     ]
    }
   ],
   "source": [
    "sim = np.matmul(W, np.transpose(W))\n",
    "print(sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Forward-backward retraining of recurrent neural networks\n",
      "--- top 10 similar docs ---\n",
      "(0.05010) Context-Dependent Multiple Distribution Phonetic Modeling with MLPs\n",
      "(0.04715) Is Learning The n-th Thing Any Easier Than Learning The First?\n",
      "(0.04123) Learning Statistically Neutral Tasks without Expert Guidance\n",
      "(0.04110) Combining Visual and Acoustic Speech Signals with a Neural Network Improves Intelligibility\n",
      "(0.04087) The Ni1000: High Speed Parallel VLSI for Implementing Multilayer Perceptrons\n",
      "(0.04038) Subset Selection and Summarization in Sequential Data\n",
      "(0.04003) Back Propagation is Sensitive to Initial Conditions\n",
      "(0.03939) Semi-Supervised Multitask Learning\n",
      "(0.03862) SoundNet: Learning Sound Representations from Unlabeled Video\n"
     ]
    }
   ],
   "source": [
    "def similar_docs(filename, sim, topn):\n",
    "    doc_id = int(filename.split(\".\")[0])\n",
    "    row = sim[doc_id, :]\n",
    "    target_docs = np.argsort(-row)[0:topn].tolist()\n",
    "    scores = row[target_docs].tolist()\n",
    "    target_filenames = [\"{:d}.txt\".format(x) for x in target_docs]\n",
    "    return target_filenames, scores\n",
    "    \n",
    "\n",
    "filename2title = {}\n",
    "with open(PAPERS_METADATA, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        cols = line.strip().split(\"\\t\")\n",
    "        filename2title[\"{:s}.txt\".format(cols[0])] = cols[2]\n",
    "\n",
    "source_filename = \"1032.txt\"\n",
    "top_n = 10\n",
    "target_filenames, scores = similar_docs(source_filename, sim, top_n)\n",
    "print(\"Source: {:s}\".format(filename2title[source_filename]))\n",
    "print(\"--- top {:d} similar docs ---\".format(top_n))\n",
    "for target_filename, score in zip(target_filenames, scores):\n",
    "    if target_filename == source_filename:\n",
    "        continue\n",
    "    print(\"({:.5f}) {:s}\".format(score, filename2title[target_filename]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggesting Documents based on Read Collection\n",
    "\n",
    "We consider an arbitary set of documents that we know a user has read or liked or marked somehow, and we want to recommend other documents that he may like.\n",
    "\n",
    "To do this, we compute the average feature among these documents (starting from the sparse features) convert it to a average dense feature vector, then find the most similar compared to that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation: 1.0\n",
      "violation: 0.23129634545431624\n",
      "violation: 0.03209572604136983\n",
      "violation: 0.007400997221153011\n",
      "violation: 0.0012999049199094925\n",
      "violation: 0.0001959522250959198\n",
      "violation: 4.179248920879007e-05\n",
      "Converged at iteration 7\n",
      "--- Source collection ---\n",
      "A Generic Approach for Identification of Event Related Brain Potentials via a Competitive Neural Network Structure\n",
      "Implicit Surfaces with Globally Regularised and Compactly Supported Basis Functions\n",
      "Learning Trajectory Preferences for  Manipulators via Iterative Improvement\n",
      "Statistical Modeling of Cell Assemblies Activities in Associative Cortex of Behaving Monkeys\n",
      "Learning to Traverse Image Manifolds\n",
      "--- Recommendations ---\n",
      "(0.06628) Fast Second Order Stochastic Backpropagation for Variational Inference\n",
      "(0.06128) Scalable Model Selection for Belief Networks\n",
      "(0.05793) Large Margin Discriminant Dimensionality Reduction in Prediction Space\n",
      "(0.05643) Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis\n",
      "(0.05629) Recognizing Activities by Attribute Dynamics\n",
      "(0.05622) Efficient Match Kernel between Sets of Features for Visual Recognition\n",
      "(0.05565) Learning Wake-Sleep Recurrent Attention Models\n",
      "(0.05466) Boosting Density Estimation\n",
      "(0.05422) Sparse deep belief net model for visual area V2\n",
      "(0.05350) Cluster Kernels for Semi-Supervised Learning\n"
     ]
    }
   ],
   "source": [
    "collection_size = np.random.randint(3, high=10, size=1)[0]\n",
    "collection_ids = np.random.randint(0, high=num_docs+1, size=collection_size)\n",
    "\n",
    "feat_vec = np.zeros((1, 11992))\n",
    "for collection_id in collection_ids:\n",
    "    feat_vec += X[collection_id, :]\n",
    "feat_vec /= collection_size\n",
    "y = model.transform(feat_vec)\n",
    "doc_sims = np.matmul(W, np.transpose(y)).squeeze(axis=1)\n",
    "target_ids = np.argsort(-doc_sims)[0:top_n]\n",
    "scores = doc_sims[target_ids]\n",
    "\n",
    "print(\"--- Source collection ---\")\n",
    "for collection_id in collection_ids:\n",
    "    print(\"{:s}\".format(filename2title[\"{:d}.txt\".format(collection_id)]))\n",
    "print(\"--- Recommendations ---\")\n",
    "for target_id, score in zip(target_ids, scores):\n",
    "    print(\"({:.5f}) {:s}\".format(score, filename2title[\"{:d}.txt\".format(target_id)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
