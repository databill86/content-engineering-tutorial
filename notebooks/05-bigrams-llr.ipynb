{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams - Log Likelihood Ratio\n",
    "\n",
    "First suggested by Ted Dunning, see [blog post](http://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html).\n",
    "\n",
    "Part of family of algorithms that finds collocations on basis of probability, i.e., a bigram is a keyword if the probability of the two words co-occurring is high compared with the probability of them appearing independently. So:\n",
    "\n",
    "$$P(B|A) \\gg P(A)P(B)$$\n",
    "\n",
    "Other algorithms in this family are (see [Foundations of Statistical Natural Language Processing, Chapter 5](https://nlp.stanford.edu/fsnlp/promo/colloc.pdf) by Manning and Schutze.\n",
    "\n",
    "* t-test\n",
    "* Chi-squared test\n",
    "* likelihood ratios\n",
    "* mutual information\n",
    "\n",
    "With Dunnings LLR method, we want to find the likelihood $log \\lambda$ of the likelihoods of the following two hypothesis.\n",
    "\n",
    "$$H1: P(w_2|w_1) = p = P(w_2|-w_1)$$\n",
    "\n",
    "$$H2: P(w_2|w_1) = p1 \\ne p2 = P(w_2|-w_1)$$\n",
    "\n",
    "Here H1 is the independence hypothesis and H2 is the dependence hypothesis.\n",
    "\n",
    "We can compute the probabilities $p$, $p_1$ and $p_2$ in terms of various counts as follows:\n",
    "\n",
    "$$p = \\frac{c_2}{N}$$\n",
    "\n",
    "$$p_1 = \\frac{c_{12}}{c_1}$$\n",
    "\n",
    "$$p_2 = \\frac{c_2 - c_{12}}{N - c_1}$$\n",
    "\n",
    "Assuming that words follow a binomial distribution, the log likelihood ratio is given by the following formula.\n",
    "\n",
    "$$log \\lambda = log \\frac{L(H1)}{L(H2)} = log \\frac{L(c_{12}, c_1, p) L(c_2 - c_{12}, N-c_1, p)}{L(c_{12}, c_1, p_1) L(c_2 - c_{12}, N-c_1, p_2)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$L(k, n, x) = x^k (1 - x)^{n-k}$$\n",
    "\n",
    "We will calculate the LLR for the bigrams we discovered in the `04-bigrams` notebook and saved in `candidate_bigrams.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "CANDIDATE_BIGRAMS = os.path.join(DATA_DIR, \"candidate_bigrams.tsv\")\n",
    "WORDCOUNTS_DB = os.path.join(DATA_DIR, \"wordcounts.db\")\n",
    "\n",
    "BIGRAM_KEYWORDS = os.path.join(DATA_DIR, \"bigram_keywords.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(WORDCOUNTS_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46889932\n"
     ]
    }
   ],
   "source": [
    "def count_num_rows(conn, table_name):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select count(*) as freq from {:s}\".format(table_name))\n",
    "    rows = cur.fetchone()\n",
    "    return int(rows[0])\n",
    "    cur.close()\n",
    "\n",
    "N = count_num_rows(conn, \"wordcounts\")\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_count(conn, unigram):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select count(*) as freq from wordcounts where word = ?\", [unigram])\n",
    "    rows = cur.fetchone()\n",
    "    return int(rows[0])\n",
    "    cur.close()\n",
    "    \n",
    "\n",
    "def log_likelihood(k, n, x):\n",
    "    # L(k, n, x) = x ** k * (1-x) ** (n-k)\n",
    "    # multiple if conditions to escape math.log ValueError\n",
    "    if x == 0:\n",
    "        return (n - k) * math.log(1 - x)\n",
    "    elif x == 1:\n",
    "        return k * math.log(x)\n",
    "    else:\n",
    "        return k * math.log(x) + (n - k) * math.log(1 - x)\n",
    "\n",
    "\n",
    "def log_likelihood_ratio(c1, c2, c12, p, p1, p2, N):\n",
    "    return (log_likelihood(c12, c1, p) + log_likelihood(c2 - c12, N - c1, p) -\n",
    "        log_likelihood(c12, c1, p1) - log_likelihood(c2 - c12, N - c1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280 candidate keywords predicted\n"
     ]
    }
   ],
   "source": [
    "llrs = []\n",
    "fbig = open(CANDIDATE_BIGRAMS, \"r\")\n",
    "for line in fbig:\n",
    "    word_1, word_2, c12 = line.strip().split(\"\\t\")\n",
    "    c12 = int(c12)\n",
    "    c1 = unigram_count(conn, word_1)\n",
    "    c2 = unigram_count(conn, word_2)\n",
    "    p = c2 / N\n",
    "    p1 = c12 / c1\n",
    "    p2 = (c2 - c12) / (N - c1)\n",
    "    llr = log_likelihood_ratio(c1, c2, c12, p, p1, p2, N)\n",
    "#     print(word_1, word_2, llr)\n",
    "    llrs.append((word_1, word_2, llr))\n",
    "\n",
    "fbig.close()\n",
    "\n",
    "print(\"{:d} candidate keywords predicted\".format(len(llrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering bigrams by LLR\n",
    "\n",
    "LLR is ratio of likelihood of independence by likelihood of dependence, so more probable bigrams means lower values of LLR, hence ordering should be by LLR ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "et al -151518.48289648563\n",
      "machine learning -74404.30454820918\n",
      "neural networks -56336.203218498355\n",
      "processing systems -54158.87871827083\n",
      "information processing -50136.83927513713\n",
      "international conference -46681.889256491806\n",
      "neural information -37865.34878083441\n",
      "neural network -32324.2155052608\n",
      "arxiv preprint -31963.201131558242\n",
      "mit press -29930.99386955142\n",
      "monte carlo -29890.898612344055\n"
     ]
    }
   ],
   "source": [
    "sorted_llrs = sorted(llrs, key=operator.itemgetter(2))\n",
    "fbk = open(BIGRAM_KEYWORDS, \"w\")\n",
    "i = 0\n",
    "for word_1, word_2, llr in sorted_llrs:\n",
    "    if i <= 10:\n",
    "        print(word_1, word_2, llr)\n",
    "    fbk.write(\"{:s}\\t{:s}\\t{:.3f}\\n\".format(word_1, word_2, llr))\n",
    "    i += 1\n",
    "    \n",
    "fbk.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
