{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and Filter ORG keywords\n",
    "\n",
    "Both Stanford and SpaCy NERs aggressively find and return ORG entities. We tried to reduce the recall by restricting to the first 50 lines for the text files. We still find see extraneous words in the entities.\n",
    "\n",
    "First thing we do is construct a term frequency dictionary from the words and find the most frequent words used, then select a subset of these words as good indicators that the ORG strings extracted are what we want.\n",
    "\n",
    "The output is better, but still quite noisy. So we will use two third party dictionaries, one of [world's top universities](https://github.com/endSly/world-universities-csv) and another of the [world's top 2000 companies](https://www.someka.net/excel-template/forbes-global-2000-list-2017/) according to Forbes, and extract them into a Aho-Corasick trie structure. If an ORG string passes throgh the first layer, if there is a match with the university or company name, then we will add to the list of ORGs for the document the _dictionary entry (not the actual ORG selected)_. Hopefully this will enable us to keep our ORG facets clean and also allow for reliable similarity calculations based on ORG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ahocorasick\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "TEXTFILES_DIR = os.path.join(DATA_DIR, \"textfiles\")\n",
    "STANFORD_ORGS_DIR = os.path.join(DATA_DIR, \"orgs_stanford\")\n",
    "SPACY_ORGS_DIR = os.path.join(DATA_DIR, \"orgs_spacy\")\n",
    "\n",
    "DICT_UNIVS = os.path.join(DATA_DIR, \"world-universities.csv\")\n",
    "DICT_CORPS = os.path.join(DATA_DIR, \"forbes-global-2000-company-list.csv\")\n",
    "\n",
    "MERGED_FILTERED_ORGS_DIR = os.path.join(DATA_DIR, \"orgs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plausible ORG keywords\n",
    "\n",
    "We run through the words and look for words that occur frequently. These would become keywords which we will use to weed out bad ORGs suggested by Stanford and SpaCy NERs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orgs(orgfile):\n",
    "    lines = []\n",
    "    forg = open(orgfile, \"r\")\n",
    "    for line in forg:\n",
    "        lines.append(line.strip())\n",
    "    return lines\n",
    "\n",
    "\n",
    "ctr = collections.Counter()\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "for textfile in os.listdir(TEXTFILES_DIR):\n",
    "    doc_id = int(textfile.split(\".\")[0])\n",
    "    stanford_orgs = get_orgs(os.path.join(STANFORD_ORGS_DIR, \"{:d}.org\".format(doc_id)))\n",
    "    spacy_orgs = get_orgs(os.path.join(SPACY_ORGS_DIR, \"{:d}.org\".format(doc_id)))\n",
    "    for org in stanford_orgs + spacy_orgs:\n",
    "        words = org.lower().split(\" \")\n",
    "        for word in words:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            ctr[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 28406),\n",
       " ('university', 14205),\n",
       " ('department', 7207),\n",
       " ('science', 6184),\n",
       " ('computer', 5806),\n",
       " ('institute', 4249),\n",
       " ('abstract', 3091),\n",
       " ('engineering', 2674),\n",
       " ('research', 2479),\n",
       " ('&', 2295),\n",
       " ('technology', 2066),\n",
       " ('neural', 1677),\n",
       " ('california', 1596),\n",
       " ('stanford', 1424),\n",
       " ('school', 1351),\n",
       " ('information', 1299),\n",
       " ('systems', 1269),\n",
       " ('electrical', 1233),\n",
       " ('learning', 1188),\n",
       " ('berkeley', 1172),\n",
       " ('center', 1126),\n",
       " ('sciences', 994),\n",
       " ('statistics', 948),\n",
       " ('cambridge', 846),\n",
       " ('college', 813),\n",
       " ('laboratory', 810),\n",
       " ('ca', 806),\n",
       " ('usa', 782),\n",
       " ('princeton', 759),\n",
       " ('markov', 747),\n",
       " ('pca', 743),\n",
       " ('mellon', 721),\n",
       " ('carnegie', 715),\n",
       " ('massachusetts', 676),\n",
       " ('microsoft', 651),\n",
       " ('processing', 641),\n",
       " ('computational', 623),\n",
       " ('toronto', 595),\n",
       " ('national', 567),\n",
       " ('neuroscience', 566),\n",
       " ('machine', 563),\n",
       " ('new', 551),\n",
       " ('mit', 533),\n",
       " (',', 532),\n",
       " ('google', 529),\n",
       " ('computing', 523),\n",
       " ('london', 512),\n",
       " ('dept.', 510),\n",
       " ('psychology', 499),\n",
       " ('group', 494)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words found yield interesting words that we can use to clean up the ORGS found from Stanford and SpaCy NERs.\n",
    "\n",
    "    institutions: \n",
    "        university, department, school, laboratory, laboratories, group, unit, division, \n",
    "        dept, center, college, \n",
    "    subjects:\n",
    "        engineering, science, sciences, computer,  mathematics, physics, biological, \n",
    "        medical, statistics, psychology,\n",
    "    famous colleges:\n",
    "        stanford, berkeley, cambridge, princeton, carnegie, massachussets, national, \n",
    "        toronto, mit, london, pittsburgh, columbia, hebrew\n",
    "    famous companies:\n",
    "        google, microsoft,\n",
    "    other ORG keywords:\n",
    "        research, technology, learning, systems, computational, computing, \n",
    "        computation, recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = set([\n",
    "    \"university\", \"college\",\n",
    "    \"department\", \"school\", \"laboratory\", \"laboratories\", \n",
    "    \"group\", \"unit\", \"division\", \"dept\", \"center\", \n",
    "    \"engineering\", \"science\", \"sciences\", \"computer\", \"mathematics\", \"physics\", \n",
    "    \"biological\", \"medical\", \"statistics\", \"psychology\",\n",
    "    \"stanford\", \"berkeley\", \"cambridge\", \"princeton\", \"carnegie\", \"massachussets\", \n",
    "    \"national\", \"toronto\", \"mit\", \"london\", \"pittsburgh\", \"columbia\", \"hebrew\",\n",
    "    \"google\", \"microsoft\", \"bell\",\n",
    "    \"research\", \"technology\", \"systems\", \"computational\", \"computing\", \n",
    "    \"computation\", \"recognition\",\n",
    "    \"learning\", \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suguru Arimoto Osaka University\n",
      "University, Toyonaka\n",
      "Stanford University\n",
      "Stanford\n",
      "National Science Foundation\n",
      "American Institute of Physics\n",
      "Stanford University\n",
      "American Institute of Physics\n",
      "the National Science Foundation\n",
      "Stanford\n",
      "AT&T Bell Labs\n"
     ]
    }
   ],
   "source": [
    "def contains_common_word(org_str, punct_table, common_words):\n",
    "    words = set([w.translate(punct_table) for w in org_str.lower().split(\" \")])\n",
    "    return len(words.intersection(common_words)) > 0\n",
    "\n",
    "punct_table = str.maketrans({key: None for key in string.punctuation})\n",
    "i = 0\n",
    "for textfile in os.listdir(TEXTFILES_DIR):\n",
    "    doc_id = int(textfile.split(\".\")[0])\n",
    "    stanford_orgs = get_orgs(os.path.join(STANFORD_ORGS_DIR, \"{:d}.org\".format(doc_id)))\n",
    "    spacy_orgs = get_orgs(os.path.join(SPACY_ORGS_DIR, \"{:d}.org\".format(doc_id)))\n",
    "    for org in stanford_orgs + spacy_orgs:\n",
    "        if contains_common_word(org, punct_table, common_words):\n",
    "            if i <= 30:\n",
    "                print(org)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Dictionary Filter\n",
    "\n",
    "This does a good job of producing clean ORG annotations, but also loses some standard names, such as The Hebrew University, Caltech, etc. We can manually scan the misses and add them manually to the automaton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built dictionary trie with 11275 entries\n"
     ]
    }
   ],
   "source": [
    "org_dict = ahocorasick.Automaton()\n",
    "idx = 0\n",
    "with open(DICT_UNIVS, \"r\") as f:\n",
    "    csv_reader = csv.DictReader(f, delimiter=\",\", \n",
    "                                fieldnames=[\"abbr\", \"name\", \"url\"])\n",
    "    for row in csv_reader:\n",
    "        org_dict.add_word(row[\"name\"], (idx, row[\"name\"]))\n",
    "        idx += 1\n",
    "\n",
    "with open(DICT_CORPS, \"r\") as f:\n",
    "    csv_reader = csv.DictReader(f, delimiter=\",\")\n",
    "    for row in csv_reader:\n",
    "        org_dict.add_word(row[\"Company Name\"], (idx, row[\"Company Name\"]))\n",
    "        idx += 1\n",
    "\n",
    "org_dict.make_automaton()\n",
    "print(\"built dictionary trie with {:d} entries\".format(len(org_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Osaka University']\n"
     ]
    }
   ],
   "source": [
    "def contains_name(org_str, org_dict):\n",
    "    return [item[1][1] for item in org_dict.iter(org_str)]\n",
    "\n",
    "print(contains_name(\"Suguru Arimoto Osaka University\", org_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply filter to all keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MERGED_FILTERED_ORGS_DIR):\n",
    "    os.mkdir(MERGED_FILTERED_ORGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 org files written\n",
      "1000 org files written\n",
      "2000 org files written\n",
      "3000 org files written\n",
      "4000 org files written\n",
      "5000 org files written\n",
      "6000 org files written\n",
      "7000 org files written\n",
      "7238 org files written, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "num_written = 0\n",
    "for textfile in os.listdir(TEXTFILES_DIR):\n",
    "    if num_written % 1000 == 0:\n",
    "        print(\"{:d} org files written\".format(num_written))\n",
    "    doc_id = int(textfile.split(\".\")[0])\n",
    "    mf_orgfile = os.path.join(MERGED_FILTERED_ORGS_DIR, \"{:d}.org\".format(doc_id))\n",
    "    if os.path.exists(mf_orgfile):\n",
    "        num_written += 1\n",
    "        continue\n",
    "    stanford_orgs = get_orgs(os.path.join(STANFORD_ORGS_DIR, \"{:d}.org\".format(doc_id)))\n",
    "    spacy_orgs = get_orgs(os.path.join(SPACY_ORGS_DIR, \"{:d}.org\".format(doc_id)))\n",
    "    fout = open(mf_orgfile, \"w\")\n",
    "    already_seen = set()\n",
    "    for org in stanford_orgs + spacy_orgs:\n",
    "        if not contains_common_word(org, punct_table, common_words):\n",
    "            continue\n",
    "        org_entities = contains_name(org, org_dict)\n",
    "        for org_entity in org_entities:\n",
    "            if org_entity in already_seen:\n",
    "                continue\n",
    "            fout.write(\"{:s}\\n\".format(org_entity))\n",
    "            already_seen.add(org_entity)\n",
    "    fout.close()\n",
    "    num_written += 1\n",
    "\n",
    "print(\"{:d} org files written, COMPLETE\".format(num_written))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
