{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts\n",
    "\n",
    "We pull out word counts using a SpaCy tokenizer (smarter than just separating by space) and write out into a SQLite3 (relational) database. The database table has columns to support a few more things down the line, but this notebook is only for word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import sqlite3\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "TEXTFILES_DIR = os.path.join(DATA_DIR, \"textfiles\")\n",
    "\n",
    "TOKENS_FILE = os.path.join(DATA_DIR, \"tokens.tsv\")\n",
    "WORDCOUNT_DB = os.path.join(DATA_DIR, \"wordcounts.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text and dump into flat file\n",
    "\n",
    "Tokenizing the text with SpaCy is a slower process than simply splitting on whitespace, but hopefully the accuracy is worth the time taken. We will pretend that this step is already done, and use the flat file tokens.tsv that we downloaded instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_content(filename):\n",
    "    lines = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            lines.append(line)\n",
    "    lines = [line+\".\" if not line.endswith(\".\") else line for line in lines]\n",
    "    return \" \".join(lines)\n",
    "\n",
    "if not os.path.exists(TOKENS_FILE):\n",
    "    # LONG PROCESS! only run if not run before, ie, if no entries in DB table\n",
    "    fout = open(TOKENS_FILE, \"w\")\n",
    "    textfiles = os.listdir(TEXTFILES_DIR)\n",
    "    total_docs = len(textfiles)\n",
    "    num_words, num_docs = 0, 0\n",
    "    for textfile in textfiles:\n",
    "        if num_docs % 500 == 0:\n",
    "            print(\"{:d}/{:d} documents processed, extracted {:d} tokens\"\n",
    "                 .format(num_docs, total_docs, num_words))\n",
    "        doc_id = int(textfile.split(\".\")[0])\n",
    "        filename = os.path.join(TEXTFILES_DIR, textfile)\n",
    "        text = read_content(filename)\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            fout.write(\"{:d}\\t{:s}\\t{:d}\\n\".format(num_words, token.text.lower(), doc_id))\n",
    "            num_words += 1\n",
    "        num_docs += 1\n",
    "    print(\"{:d}/{:d} documents processed, extracted {:d} tokens, COMPLETE\"\n",
    "          .format(num_docs, total_docs, num_words))\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading to Database table\n",
    "\n",
    "We use a lightweight RDBMS (SQLite3) to offload some of the memory load (for example, if we were trying to build an in-memory dictionary of words to counts). Specifically, we just insert all the raw data we downloaded to file.\n",
    "\n",
    "We then create some indexes to speed up access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(conn, table_name):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select name from sqlite_master where type='table' and name = ?\", \n",
    "                [table_name])\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return len(rows) > 0\n",
    "\n",
    "\n",
    "def create_wordcounts_table(conn):\n",
    "    if not table_exists(conn, \"wordcounts\"):\n",
    "        cur = conn.cursor()\n",
    "        create_table = \"\"\"create table wordcounts(\n",
    "            id INTEGER NOT NULL,\n",
    "            word VARCHAR(50) NOT NULL, \n",
    "            doc_id INTEGER NOT NULL)\n",
    "        \"\"\"\n",
    "        cur.execute(create_table)\n",
    "        cur.close()\n",
    "\n",
    "\n",
    "def index_exists(conn, index_name):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select name from sqlite_master where type='index' and name = ?\", \n",
    "                [index_name])\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "    return len(rows) > 0\n",
    "\n",
    "\n",
    "def create_wordcounts_indexes(conn):\n",
    "    cur = conn.cursor()\n",
    "    index_names = [\"ix_wordcounts\", \"ax1_wordcounts\", \"ax2_wordcounts\"]\n",
    "    create_indexes = [\n",
    "        \"create unique index ix_wordcounts on wordcounts(id)\",\n",
    "        \"create index ax1_wordcounts on wordcounts(word)\",\n",
    "        \"create index ax2_wordcounts on wordcounts(word, doc_id)\"\n",
    "    ]\n",
    "    for index_name, create_index in zip(index_names, create_indexes):\n",
    "        if not index_exists(conn, index_name):\n",
    "            cur.execute(create_index)\n",
    "    cur.close()\n",
    "\n",
    "\n",
    "def insert_word(conn, id, word, doc_id, commit=False):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"insert into wordcounts(id, word, doc_id) values (?, ?, ?)\", \n",
    "                [id, word, doc_id])\n",
    "    if commit:\n",
    "        conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "        \n",
    "def count_words_in_table(conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select count(*) as cnt from wordcounts\")\n",
    "    rows = cur.fetchone()\n",
    "    return int(rows[0])\n",
    "    cur.close()\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(WORDCOUNT_DB)\n",
    "create_wordcounts_table(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = count_words_in_table(conn)\n",
    "if num_words == 0:\n",
    "    num_read = 0\n",
    "    should_commit = True\n",
    "    ftoks = open(TOKENS_FILE, \"r\")\n",
    "    for line in ftoks:\n",
    "        if num_read % 1000 == 0:\n",
    "            print(\"{:d} words loaded\".format(num_read))\n",
    "            should_commit = True\n",
    "        else:\n",
    "            should_commit = False\n",
    "        cols = line.strip().split(\"\\t\")\n",
    "        try:\n",
    "            id = int(cols[0])\n",
    "            word = cols[1]\n",
    "            doc_id = int(cols[2])\n",
    "            insert_word(conn, id, word, doc_id, should_commit)\n",
    "        except ValueError:\n",
    "            # ignore them, they are spaces and special chars\n",
    "            continue \n",
    "        num_read += 1\n",
    "    ftoks.close()\n",
    "    print(\"{:d} words loaded\".format(num_read))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcounts_indexes(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common terms\n",
    "\n",
    "We pull up the 1000 terms with the highest frequency. Output shows lots of junk, for example punctuation characters, stop words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 5107669\n",
      ", 2257816\n",
      "the 1960197\n",
      "? 1510757\n",
      ") 1097449\n",
      "of 1001307\n",
      "( 884228\n",
      "and 746652\n",
      "a 661768\n",
      "in 622168\n",
      "to 593634\n",
      "- 506861\n",
      "is 504759\n",
      "for 405409\n",
      "we 351814\n",
      "] 336460\n",
      "[ 326642\n",
      "1 313542\n",
      "= 313274\n",
      "that 300599\n",
      "with 232010\n",
      ": 231680\n",
      "on 209176\n",
      "as 205143\n",
      "2 203857\n",
      "by 195557\n",
      "this 189965\n",
      "are 184099\n",
      "be 172591\n",
      "0 139209\n",
      "can 132284\n",
      "an 127718\n",
      "+ 127107\n",
      "from 127093\n",
      "x 120524\n",
      "t 110426\n",
      "i 108454\n",
      "which 107900\n",
      "learning 105867\n",
      "model 104103\n",
      "it 96898\n",
      "k 95817\n",
      "3 94849\n",
      "data 94142\n",
      "our 90981\n",
      "n 87813\n",
      "each 87187\n",
      "algorithm 82145\n",
      "at 81045\n",
      "4 80238\n"
     ]
    }
   ],
   "source": [
    "def word_counts(conn, top_n):\n",
    "    cur = conn.cursor()\n",
    "    count_sql = \"\"\"\n",
    "        select word, count(word) as word_count \n",
    "        from wordcounts \n",
    "        group by word \n",
    "        order by word_count desc\"\"\"\n",
    "    if top_n != -1:\n",
    "        count_sql += \" limit {:d}\".format(top_n)\n",
    "    cur.execute(count_sql)\n",
    "    rows = cur.fetchall()\n",
    "    return [(row[0], row[1]) for row in rows]\n",
    "    \n",
    "top_n = word_counts(conn, 50)\n",
    "for word, count in top_n:\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple filters\n",
    "\n",
    "We set up a few simple filters to get rid of the obvious noise and see what our top terms are. As expected, top terms do correspond to Deep Learning stuff -- with learning, model, data and algorithm the top 4 words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def is_number_or_fraction(word):\n",
    "    return re.match(r\"(\\d)+(\\.(\\d)+)*\", word) is not None\n",
    "\n",
    "\n",
    "def is_all_punctuation(word):\n",
    "    chars = [c for c in word if c not in string.punctuation]\n",
    "    return len(chars) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning 105867\n",
      "model 104103\n",
      "data 94142\n",
      "algorithm 82145\n"
     ]
    }
   ],
   "source": [
    "for word, count in top_n:\n",
    "    if len(word) < 2:\n",
    "        continue\n",
    "    if len(word) == 2 and word.endswith(\".\"):\n",
    "        continue\n",
    "    if is_number_or_fraction(word):\n",
    "        continue\n",
    "    if is_all_punctuation(word):\n",
    "        continue\n",
    "    if word in english_stopwords:\n",
    "        continue\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
