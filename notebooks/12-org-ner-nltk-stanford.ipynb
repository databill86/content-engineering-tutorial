{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting ORGs from papers using NLTK and Stanford NER\n",
    "\n",
    "This notebook is based on the blog post [Named Entity Recognition with Stanford NER Tagger](https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/) by Chuck Dishmon.\n",
    "\n",
    "We scan the text of the papers looking for organization names, and use them in a manner similar to keywords and authors, ie, yet another facet (or feature for content similarity going forward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "MODELS_DIR = \"../models\"\n",
    "\n",
    "STANFORD_MODELS = os.path.join(MODELS_DIR, \"stanford-ner-2018-02-27\")\n",
    "\n",
    "STANFORD_NER_MODEL = os.path.join(STANFORD_MODELS, \"stanford-ner.jar\")\n",
    "STANFORD_CRF_MODEL = os.path.join(STANFORD_MODELS, \"classifiers\",\n",
    "                                  \"english.all.3class.distsim.crf.ser.gz\")\n",
    "\n",
    "TEXTFILES_DIR = os.path.join(DATA_DIR, \"textfiles\")\n",
    "\n",
    "TOP_N_LINES = 50\n",
    "TEXTFILES_ORG_DIR = os.path.join(DATA_DIR, \"textfiles_org\")\n",
    "\n",
    "ORGS_STANFORD_RAW = os.path.join(DATA_DIR, \"orgs_stanford_raw\")\n",
    "\n",
    "ORGS_STANFORD_DIR = os.path.join(DATA_DIR, \"orgs_stanford\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extractor\n",
    "\n",
    "The NLTK StanfordNERTagger tagger wraps the Stanford NER Java model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = nltk.tag.StanfordNERTagger(STANFORD_CRF_MODEL, STANFORD_NER_MODEL, \n",
    "                                    encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yann Le Cun, a native of France was not even 30 when he joined AT&T Bell Laboratories in New Jersey. At Bell Labs, LeCun developed a number of new machine learning methods, including the convolutional neural network—modeled after the visual cortex in animals. Today, he serves as chief AI scientist at Facebook, where he works tirelessly towards new breakthroughs.\n",
      "['AT & T Bell Laboratories', 'Bell Labs', 'Facebook']\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(tagger, text, debug=False):\n",
    "    entities, entity = [], []\n",
    "    for sid, sent in enumerate(nltk.sent_tokenize(text)):\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tagged = tagger.tag(tokens)\n",
    "        prev_tid = None\n",
    "        for tid, (token, tag) in enumerate(tagged):\n",
    "            if tag == \"ORGANIZATION\":\n",
    "                if debug:\n",
    "                    print(sid, tid, token, tag)\n",
    "                if prev_tid is None or prev_tid + 1 < tid:\n",
    "                    if len(entity) > 0:\n",
    "                        entities.append(\" \".join(entity))\n",
    "                    entity = []\n",
    "                entity.append(token)\n",
    "                prev_tid = tid\n",
    "    if len(entity) > 0:\n",
    "        entities.append(\" \".join(entity))\n",
    "    return entities\n",
    "\n",
    "\n",
    "text = \"\"\"Yann Le Cun, a native of France was not even 30 when he joined AT&T \n",
    "Bell Laboratories in New Jersey. At Bell Labs, LeCun developed a number of new \n",
    "machine learning methods, including the convolutional neural network—modeled \n",
    "after the visual cortex in animals. Today, he serves as chief AI scientist at\n",
    "Facebook, where he works tirelessly towards new breakthroughs.\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "text = re.sub(\"\\s+\", \" \", text)\n",
    "print(text)\n",
    "\n",
    "entities = extract_entities(tagger, text)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text files\n",
    "\n",
    "Our intent is to basically look only at the top few lines of the paper to find organizations that the authors are affiliated with. So we don't want to parse the full paper, because of (a) processing time and (b) noisy entities.\n",
    "\n",
    "Based on looking at a few papers, it seems that this information can be found within the top 50 lines (`TOP_N_LINES`), so we will use that as a cutoff. This is preferable to looking for magic section names such as ABSTRACT or INTRODUCTION and all the misspellings. Also this allows us to save time on non-English papers, where our standard models would not work anyway.\n",
    "\n",
    "This step will read the files from textfile and write out to `TEXTFILES_ORG_DIR` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TEXTFILES_ORG_DIR):\n",
    "    os.mkdir(TEXTFILES_ORG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 headers written\n",
      "1000 headers written\n",
      "2000 headers written\n",
      "3000 headers written\n",
      "4000 headers written\n",
      "5000 headers written\n",
      "6000 headers written\n",
      "7000 headers written\n",
      "7238 headers written, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "def get_text(textfile, top_n):\n",
    "    lines = []\n",
    "    i = 0\n",
    "    f = open(textfile, \"r\")\n",
    "    for line in f:\n",
    "        lines.append(line.strip())\n",
    "        i += 1\n",
    "        if i > top_n:\n",
    "            break\n",
    "    f.close()\n",
    "    text = \"\\n\".join(lines)\n",
    "    return text\n",
    "\n",
    "\n",
    "num_written = 0\n",
    "for textfile in os.listdir(TEXTFILES_DIR):\n",
    "    if num_written % 1000 == 0:\n",
    "        print(\"{:d} headers written\".format(num_written))\n",
    "    doc_id = int(textfile.split(\".\")[0])\n",
    "    head_file = os.path.join(TEXTFILES_ORG_DIR, \"{:d}.txt\".format(doc_id))\n",
    "    if os.path.exists(head_file):\n",
    "        num_written += 1\n",
    "        continue\n",
    "    text = get_text(os.path.join(TEXTFILES_DIR, textfile), TOP_N_LINES)\n",
    "    fhead = open(head_file, \"w\")\n",
    "    fhead.write(text)\n",
    "    fhead.close()\n",
    "    num_written += 1\n",
    "\n",
    "print(\"{:d} headers written, COMPLETE\".format(num_written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ORG extractor for all files\n",
    "\n",
    "__NOTE:__ running the above code on the contents of TEXTFILES_DIR is too slow. It is faster to execute the Java command on the individual files using a shell script and processing the output to extract ORGs. Command to run the extractor from shell:\n",
    "\n",
    "    cd ../models/stanford-ner-2018-02-27\n",
    "    java -mx600m -cp \"*:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier \\\n",
    "        -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz \\\n",
    "        -outputFormat tabbedEntities \\\n",
    "        -textFile ../../data/textfiles_org/1.txt > ../../data/orgs_stanford_raw/1.org\n",
    "\n",
    "Finally, we will run the generated script to get Stanford NER output in `ORGS_STANFORD_RAW`.\n",
    "\n",
    "    ./my_script.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ORGS_STANFORD_RAW):\n",
    "    os.mkdir(ORGS_STANFORD_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -mx600m -cp \"*:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -outputFormat tabbedEntities -textFile ../../data/textfiles_org/1.txt > ../../data/orgs_stanford_raw/1.org\n"
     ]
    }
   ],
   "source": [
    "def create_script(template, script_file, textfiles_dir):\n",
    "    fscript = open(script_file, \"w\")\n",
    "    for textfile in os.listdir(textfiles_dir):\n",
    "        doc_id = int(textfile.split(\".\")[0])\n",
    "        command = template.format(doc_id, doc_id)\n",
    "        fscript.write(\"{:s}\\n\".format(command))\n",
    "    fscript.close()\n",
    "\n",
    "\n",
    "template = \"\"\"java -mx600m -cp \"*:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier\n",
    "    -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz\n",
    "    -outputFormat tabbedEntities\n",
    "    -textFile ../../data/textfiles_org/{:d}.txt > ../../data/orgs_stanford_raw/{:d}.org\"\"\"\n",
    "template = template.replace(\"\\n\", \" \")\n",
    "template = re.sub(\"\\s+\", \" \", template)\n",
    "template = template.lstrip().rstrip()\n",
    "print(template.format(1, 1))\n",
    "\n",
    "create_script(template, os.path.join(STANFORD_MODELS, \"my_script.sh\"), TEXTFILES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse script output\n",
    "\n",
    "Just pull out the tokens tagged as ORGANIZATION from the raw files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 org files written\n",
      "1000 org files written\n",
      "2000 org files written\n",
      "3000 org files written\n",
      "4000 org files written\n",
      "5000 org files written\n",
      "6000 org files written\n",
      "7000 org files written\n",
      "7238 org files written, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(ORGS_STANFORD_DIR):\n",
    "    os.mkdir(ORGS_STANFORD_DIR)\n",
    "    \n",
    "num_written = 0\n",
    "for rawfile in os.listdir(ORGS_STANFORD_RAW):\n",
    "    if num_written % 1000 == 0:\n",
    "        print(\"{:d} org files written\".format(num_written))\n",
    "    orgfile = os.path.join(ORGS_STANFORD_DIR, rawfile)\n",
    "    if os.path.exists(orgfile):\n",
    "        num_written += 1\n",
    "        continue\n",
    "    forg = open(orgfile, \"w\")\n",
    "    fraw = open(os.path.join(ORGS_STANFORD_RAW, rawfile), \"r\")\n",
    "    for line in fraw:\n",
    "        try:\n",
    "            token, tag, _ = line.strip().split(\"\\t\", 3)\n",
    "            if tag == \"ORGANIZATION\":\n",
    "                forg.write(\"{:s}\\n\".format(token))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    num_written += 1\n",
    "    forg.close()\n",
    "    fraw.close()\n",
    "\n",
    "print(\"{:d} org files written, COMPLETE\".format(num_written))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
