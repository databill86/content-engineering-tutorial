{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting ORGs from papers using NLTK and Stanford NER\n",
    "\n",
    "This notebook is based on the blog post [Named Entity Recognition with Stanford NER Tagger](https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/) by Chuck Dishmon.\n",
    "\n",
    "We scan the text of the papers looking for organization names, and use them in a manner similar to keywords and authors, ie, yet another facet (or feature for content similarity going forward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "MODELS_DIR = \"../models\"\n",
    "\n",
    "STANFORD_MODELS = os.path.join(MODELS_DIR, \"stanford-ner-2018-02-27\")\n",
    "\n",
    "STANFORD_NER_MODEL = os.path.join(STANFORD_MODELS, \"stanford-ner.jar\")\n",
    "STANFORD_CRF_MODEL = os.path.join(STANFORD_MODELS, \"classifiers\",\n",
    "                                  \"english.all.3class.distsim.crf.ser.gz\")\n",
    "\n",
    "TEXTFILES_DIR = os.path.join(DATA_DIR, \"textfiles\")\n",
    "ORGS_STANFORD_DIR = os.path.join(DATA_DIR, \"orgs_stanford\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extractor\n",
    "\n",
    "The NLTK StanfordNERTagger tagger wraps the Stanford NER Java model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = nltk.tag.StanfordNERTagger(STANFORD_CRF_MODEL, STANFORD_NER_MODEL, \n",
    "                                    encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yann Le Cun, a native of France was not even 30 when he joined AT&T Bell Laboratories in New Jersey. At Bell Labs, LeCun developed a number of new machine learning methods, including the convolutional neural network—modeled after the visual cortex in animals. Today, he serves as chief AI scientist at Facebook, where he works tirelessly towards new breakthroughs.\n",
      "['AT & T Bell Laboratories', 'Bell Labs', 'Facebook']\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(tagger, text, debug=False):\n",
    "    entities, entity = [], []\n",
    "    for sid, sent in enumerate(nltk.sent_tokenize(text)):\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tagged = tagger.tag(tokens)\n",
    "        prev_tid = None\n",
    "        for tid, (token, tag) in enumerate(tagged):\n",
    "            if tag == \"ORGANIZATION\":\n",
    "                if debug:\n",
    "                    print(sid, tid, token, tag)\n",
    "                if prev_tid is None or prev_tid + 1 < tid:\n",
    "                    if len(entity) > 0:\n",
    "                        entities.append(\" \".join(entity))\n",
    "                    entity = []\n",
    "                entity.append(token)\n",
    "                prev_tid = tid\n",
    "    if len(entity) > 0:\n",
    "        entities.append(\" \".join(entity))\n",
    "    return entities\n",
    "\n",
    "\n",
    "text = \"\"\"Yann Le Cun, a native of France was not even 30 when he joined AT&T \n",
    "Bell Laboratories in New Jersey. At Bell Labs, LeCun developed a number of new \n",
    "machine learning methods, including the convolutional neural network—modeled \n",
    "after the visual cortex in animals. Today, he serves as chief AI scientist at\n",
    "Facebook, where he works tirelessly towards new breakthroughs.\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "text = re.sub(\"\\s+\", \" \", text)\n",
    "print(text)\n",
    "\n",
    "entities = extract_entities(tagger, text)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ORG extractor for all files\n",
    "\n",
    "__NOTE:__ running the above code on the contents of TEXTFILES_DIR is too slow. It is faster to execute the Java command on the individual files using a shell script and processing the output to extract ORGs. Command to run the extractor from shell:\n",
    "\n",
    "    cd ../models/stanford-ner-2018-02-27\n",
    "    java -mx600m -cp \"*:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier \\\n",
    "        -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz \\\n",
    "        -outputFormat tabbedEntities \\\n",
    "        -textFile ../../data/textfiles/1.txt > ../../data/orgs_stanford/1.org\n",
    "    # create script (see following cell)\n",
    "    ./my_script.sh\n",
    "\n",
    "Output will be in a separate folder identified by ORGS_STANFORD_DIR. At the end of the process, we will collect these into a single file to check for sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ORGS_STANFORD_DIR):\n",
    "    os.mkdir(ORGS_STANFORD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -mx600m -cp \"*:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -outputFormat tabbedEntities -textFile ../../data/textfiles/1.txt > ../../data/orgs_stanford/1.org\n"
     ]
    }
   ],
   "source": [
    "def create_script(template, script_file, textfiles_dir):\n",
    "    fscript = open(script_file, \"w\")\n",
    "    for textfile in os.listdir(textfiles_dir):\n",
    "        doc_id = int(textfile.split(\".\")[0])\n",
    "        command = template.format(doc_id, doc_id)\n",
    "        fscript.write(\"{:s}\\n\".format(command))\n",
    "    fscript.close()\n",
    "\n",
    "\n",
    "template = \"\"\"java -mx600m -cp \"*:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier\n",
    "    -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz\n",
    "    -outputFormat tabbedEntities\n",
    "    -textFile ../../data/textfiles/{:d}.txt > ../../data/orgs_stanford/{:d}.org\"\"\"\n",
    "template = template.replace(\"\\n\", \" \")\n",
    "template = re.sub(\"\\s+\", \" \", template)\n",
    "template = template.lstrip().rstrip()\n",
    "print(template.format(1, 1))\n",
    "\n",
    "create_script(template, os.path.join(STANFORD_MODELS, \"my_script.sh\"), TEXTFILES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse script output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suguru Arimoto Osaka University\n",
      "American Institute of Physics\n",
      "ANZA\n",
      "Fa\n",
      "Preprocessing\n",
      "IBM\n",
      "Global Training\n",
      "Intell\n",
      "PAMI\n",
      "Robotics Research\n",
      "Robotics & Automation\n",
      "Robotics & Automation\n",
      "Robotics & Automation\n",
      "Robotics & Automation\n"
     ]
    }
   ],
   "source": [
    "fsout = open(os.path.join(ORGS_STANFORD_DIR, \"0.org\"), \"r\")\n",
    "for line in fsout:\n",
    "    try:\n",
    "        token, tag, _ = line.strip().split(\"\\t\")\n",
    "        if tag == \"ORGANIZATION\":\n",
    "            print(token)\n",
    "    except ValueError:\n",
    "        continue\n",
    "fsout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
