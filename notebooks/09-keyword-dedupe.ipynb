{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting near duplicate keywords using dedupe\n",
    "\n",
    "The dedupe Python library is a machine learning library that uses a combination of blocking, hierarchical clustering and logistic regression to generate clusters out of similar records. The use case it focuses on are structured records - a good overview of its use can be found in the [Basics of Entity Resolution with Python and Dedupe](https://medium.com/district-data-labs/basics-of-entity-resolution-with-python-and-dedupe-bc87440b64d4) article.\n",
    "\n",
    "Since our keywords are likely to be of different sizes, we will use 3-char shingles and feature hashing to reduce each keyword to an integer array of 25 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "CURATED_KEYWORDS = os.path.join(DATA_DIR, \"raw_keywords.txt\")\n",
    "# dedupe wants (or suggests) a CSV file\n",
    "CURATED_KEYWORD_HASHES = os.path.join(DATA_DIR, \"curated_keywords_hash.csv\")\n",
    "# output of dedupe\n",
    "KEYWORD_DEDUPE_MAPPINGS = os.path.join(DATA_DIR, \"keyword_dedupe_mappings.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode keywords\n",
    "\n",
    "For each keyword, we create 3-char shingles, then use the sklearn FeatureHasher to hash the array of shingles to a fixed length integer array. We show below that these arrays can be sent through a similarity measure such as Jaccard and return intuitively good values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('absolute value', array([ 0,  0,  1,  0,  0,  0,  0, -1, -1,  0,  0, -1,  0,  0,  0,  0,  0,\n",
      "        0,  0, -2, -1,  0,  1,  0,  0], dtype=int32))\n",
      "('absolute values', array([ 0,  0,  1,  0,  0,  0,  0, -2, -1,  0,  0, -1,  0,  0,  0,  0,  0,\n",
      "        0,  0, -2, -1,  0,  1,  0,  0], dtype=int32))\n",
      "('jaccard:', 0.96)\n"
     ]
    }
   ],
   "source": [
    "hasher = FeatureHasher(input_type=\"string\", n_features=25, dtype=np.int32)\n",
    "keywords = [\"absolute value\", \"absolute values\"]\n",
    "hashes = []\n",
    "for keyword in keywords:\n",
    "    shingles = [\"\".join(trigram) for trigram in nltk.trigrams([c for c in keyword])]\n",
    "    keyword_hash = hasher.transform([shingles]).toarray()\n",
    "    hashes.append(keyword_hash[0])\n",
    "    print(keyword, keyword_hash[0])\n",
    "\n",
    "print(\"jaccard:\", jaccard_similarity_score(hashes[0], hashes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num keywords: 2281\n"
     ]
    }
   ],
   "source": [
    "fcurated = open(CURATED_KEYWORDS, \"r\")\n",
    "fhashed = open(CURATED_KEYWORD_HASHES, \"w\")\n",
    "\n",
    "# header\n",
    "cols = [\"id\"]\n",
    "cols.extend([\"col_{:d}\".format(i+1) for i in range(25)])\n",
    "cols.append(\"keyword\")\n",
    "fhashed.write(\"{:s}\\n\".format(\",\".join(cols)))\n",
    "\n",
    "# shingle each word into 3-char trigrams, then hash to 25 features\n",
    "hasher = FeatureHasher(input_type=\"string\", n_features=25, dtype=np.int32)\n",
    "for rowid, keyword in enumerate(fcurated):\n",
    "    keyword = keyword.strip()\n",
    "    shingles = [\"\".join(trigram) for trigram in nltk.trigrams([c for c in keyword])]\n",
    "    keyword_hash = hasher.transform([shingles]).todense().tolist()[0]\n",
    "    cols = [str(rowid)]\n",
    "    cols.append(\",\".join([str(h) for h in keyword_hash]))\n",
    "    cols.append(keyword)\n",
    "    fhashed.write(\"{:s}\\n\".format(\",\".join(cols)))\n",
    "\n",
    "fhashed.close()\n",
    "fcurated.close()\n",
    "print(\"num keywords: {:d}\".format(rowid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster encoded keywords using dedupe\n",
    "\n",
    "The pipeline is closely modeled after the [CSV example in the dedup-examples repository](https://github.com/dedupeio/dedupe-examples). Code is in a script `../scripts/dedupe_keyword_train.py`. Input is the `../data/curated_keywords_hash.csv` file we generated in this notebook. Output is a settings and a labels file which is generated as a result of the active learning step the first time the model trains so you don't have to repeat the labeling exercise every time. Final output is a set of pairs similar to the one we generated using `simhash` in the previous notebook.\n",
    "\n",
    "Output looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning approaches      \tlearning approach        \t0.776\t2.000\n",
      "absolute values          \tabsolute value           \t0.796\t1.000\n",
      "dual variables           \tdual variable            \t0.878\t1.000\n",
      "synaptic weights         \tsynaptic weight          \t0.816\t1.000\n",
      "performance measures     \tperformance measure      \t0.818\t1.000\n",
      "synthetic dataset        \tsynthetic data           \t0.684\t3.000\n",
      "dynamical systems        \tdynamical system         \t0.836\t1.000\n",
      "action pairs             \taction pair              \t0.877\t1.000\n",
      "action potentials        \taction potential         \t0.853\t1.000\n",
      "learning models          \tlearning model           \t0.816\t1.000\n",
      "action spaces            \taction space             \t0.816\t1.000\n",
      "---\n",
      "accuracy: 0.889\n",
      "---\n",
      "confusion matrix\n",
      "[[ 52   5]\n",
      " [ 31 235]]\n",
      "---\n",
      "classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.91      0.74        57\n",
      "          1       0.98      0.88      0.93       266\n",
      "\n",
      "avg / total       0.92      0.89      0.90       323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "labels, preds = [], []\n",
    "f = open(KEYWORD_DEDUPE_MAPPINGS, \"r\")\n",
    "for line in f:\n",
    "    keyword_left, keyword_right, score = line.strip().split(\"\\t\")\n",
    "    score = float(score)\n",
    "    preds.append(1 if score > 0.75 else 0)\n",
    "    edit_dist = nltk.edit_distance(keyword_left, keyword_right)\n",
    "    labels.append(1 if edit_dist <= 2 else 0)\n",
    "    if i <= 10:\n",
    "        print(\"{:25s}\\t{:25s}\\t{:.3f}\\t{:.3f}\".format(keyword_left, keyword_right, \n",
    "                                                      score, edit_dist))\n",
    "    i += 1\n",
    "f.close()\n",
    "\n",
    "acc = accuracy_score(labels, preds)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "cr = classification_report(labels, preds)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"accuracy: {:.3f}\".format(acc))\n",
    "print(\"---\")\n",
    "print(\"confusion matrix\")\n",
    "print(cm)\n",
    "print(\"---\")\n",
    "print(\"classification report\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about clustering?\n",
    "\n",
    "I also tried encoding the keywords as described and recursively splitting it up with different clustering algorithms (KMeans and Spectral) until the size of the output cluster is less than some preset threshold. Unfortunately, the clustering algorithm operated by splitting off one row at a time, finally ending with N single row clusters where N is the size of the original dataset. So naive clustering is probably not the way to go with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
