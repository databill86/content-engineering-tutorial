{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Solr with keyword facets\n",
    "\n",
    "Using the keywords we just generated and deduped, we now want to add them into the documents in our index, so we can now do the following:\n",
    "\n",
    "* Show facets along with search results to allow user to drill down into the subset of search results given by the facet.\n",
    "* Allow better query parsing by using the keywords as a dictionary which can be looked up to isolate longer entities than just words.\n",
    "* Have a rudimentary notion of \"similar documents\".\n",
    "* On the content page, show \"similar queries\" that might be interesting given the facets set up for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "WORDCOUNTS_DB = os.path.join(DATA_DIR, \"wordcounts.db\")\n",
    "\n",
    "CURATED_KEYWORDS = os.path.join(DATA_DIR, \"raw_keywords.txt\")\n",
    "NEARDUP_MAPPINGS = os.path.join(DATA_DIR, \"keyword_neardup_mappings.tsv\")\n",
    "DEDUPE_MAPPINGS = os.path.join(DATA_DIR, \"keyword_dedupe_mappings.tsv\")\n",
    "\n",
    "TEXTFILES_DIR = os.path.join(DATA_DIR, \"textfiles\")\n",
    "METADATA_FILE = os.path.join(DATA_DIR, \"papers_metadata.tsv\")\n",
    "\n",
    "SOLR_URL = \"http://localhost:8983/solr/nips1index/\"\n",
    "# SOLR_URL = \"http://localhost:8983/solr/nips2index/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(WORDCOUNTS_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index with keyword facets\n",
    "\n",
    "First we create a new core to hold our new index.\n",
    "\n",
    "    cd <solr_home>\n",
    "    bin/solr create -c nips1index\n",
    "    \n",
    "Then add in our new schema. This has one additional multiValued string field keywords to hold our facets. Additionally we have changed the type of the authors field to \"string\" to allow faceting on authors as well.\n",
    "\n",
    "    cd ../scripts\n",
    "    ./create_schema1.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup tables for keywords\n",
    "\n",
    "The full list of keywords is available in the `bigrams`, `rake` and `maui` tables. But they need to be filtered using the manually curated list of keywords. Finally these keywords need to be replaced with their canonical form so they look nicer when displayed as facets. \n",
    "\n",
    "We will also do a reverse lookup on these canonical form lookup tables when working on query expansion later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2282 valid keywords\n"
     ]
    }
   ],
   "source": [
    "valid_keywords = set()\n",
    "fcurated = open(CURATED_KEYWORDS, \"r\")\n",
    "for line in fcurated:\n",
    "    valid_keywords.add(line.strip())\n",
    "fcurated.close()\n",
    "\n",
    "print(\"{:d} valid keywords\".format(len(valid_keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 raw to canonical mappings\n"
     ]
    }
   ],
   "source": [
    "raw2canonical = {}\n",
    "fneardup = open(NEARDUP_MAPPINGS, \"r\")\n",
    "for line in fneardup:\n",
    "    key, value = line.strip().split(\"\\t\")\n",
    "    raw2canonical[key] = value\n",
    "fneardup.close()\n",
    "\n",
    "fdedupe = open(DEDUPE_MAPPINGS, \"r\")\n",
    "for line in fdedupe:\n",
    "    key, value, _ = line.strip().split(\"\\t\")\n",
    "    raw2canonical[key] = value\n",
    "fdedupe.close()\n",
    "\n",
    "print(\"{:d} raw to canonical mappings\".format(len(raw2canonical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science foundation', 'total number', 'random variables', 'optimal values', 'asymptotically optimal', 'national science foundation', 'neural net', 'optimal performance', 'neural network', 'information theory', 'upper bound', 'lower bound', 'maximum number', 'associative memory', 'hamming distance', 'ieee transactions', 'computer science', 'internal representation', 'matrix multiplication', 'national science']\n"
     ]
    }
   ],
   "source": [
    "def get_keywords_for_doc(conn, doc_id, valid_keywords, raw2canonical):\n",
    "    keywords = set()\n",
    "    # collect from bigrams\n",
    "    cur_bigrams = conn.cursor()\n",
    "    cur_bigrams.execute(\"select word_1, word_2 from bigrams where doc_id = ?\", [doc_id])\n",
    "    rows = cur_bigrams.fetchall()\n",
    "    for row in rows:\n",
    "        keyword = \" \".join([row[0], row[1]]).lower()\n",
    "        keywords.add(keyword)\n",
    "    cur_bigrams.close()\n",
    "    # collect from rake\n",
    "    cur_rake = conn.cursor()\n",
    "    cur_rake.execute(\"select keyword from rake where doc_id = ?\", [doc_id])\n",
    "    rows = cur_rake.fetchall()\n",
    "    for row in rows:\n",
    "        keywords.add(row[0].lower())\n",
    "    cur_rake.close()\n",
    "    # collect from maui\n",
    "    cur_maui = conn.cursor()\n",
    "    cur_maui.execute(\"select keyword from maui where doc_id = ?\", [doc_id])\n",
    "    rows = cur_maui.fetchall()\n",
    "    for row in rows:\n",
    "        keywords.add(row[0].lower())\n",
    "    cur_maui.close()\n",
    "    # filter out valid keywords\n",
    "    filtered_keywords = [keyword for keyword in keywords \n",
    "                         if keyword in valid_keywords]\n",
    "    # map them to their canonical forms if applicable\n",
    "    canonical_keywords = set()\n",
    "    for keyword in filtered_keywords:\n",
    "        if keyword in raw2canonical.keys():\n",
    "            canonical_keywords.add(raw2canonical[keyword])\n",
    "        else:\n",
    "            canonical_keywords.add(keyword)\n",
    "    # return as list\n",
    "    return list(canonical_keywords)\n",
    "\n",
    "\n",
    "doc_id = 1\n",
    "print(get_keywords_for_doc(conn, doc_id, valid_keywords, raw2canonical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document metadata lookup\n",
    "\n",
    "We already saved the non-text values for each document in the `papers_metadata.tsv` file in the `01-preprocess` notebook, so we will use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7238 metadata mappings\n"
     ]
    }
   ],
   "source": [
    "id2metadata = {}\n",
    "fmeta = open(METADATA_FILE, \"r\")\n",
    "for line in fmeta:\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "    id, year, title, abstract, author_names = line.split(\"\\t\")\n",
    "    authors = author_names.split(\":\")\n",
    "    id2metadata[int(id)] = (year, title, abstract, authors)\n",
    "\n",
    "fmeta.close()\n",
    "print(\"{:d} metadata mappings\".format(len(id2metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load papers into Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_index_rows(solr_url):\n",
    "    resp = requests.get(solr_url + \"select?q=*:*\")\n",
    "    resp_json = json.loads(resp.text)\n",
    "    return resp_json[\"response\"][\"numFound\"]\n",
    "\n",
    "\n",
    "def add_row(solr_url, doc_id, conn, valid_keywords, raw2canonical, \n",
    "            id2metadata, should_commit):\n",
    "    headers = {\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"accept\": \"application/json\"\n",
    "    }\n",
    "    if doc_id is None:\n",
    "        requests.post(solr_url + \"update\", params={\"commit\":\"true\"}, headers=headers)\n",
    "    else:\n",
    "        ftext = open(os.path.join(TEXTFILES_DIR, \"{:d}.txt\".format(doc_id)), \"r\")\n",
    "        textfile_lines = []\n",
    "        for line in ftext:\n",
    "            textfile_lines.append(line.strip())\n",
    "        ftext.close()\n",
    "        text = \"\\n\".join(textfile_lines)\n",
    "        year, title, abstract, authors = id2metadata[doc_id]\n",
    "        keywords = get_keywords_for_doc(conn, doc_id, valid_keywords, raw2canonical)\n",
    "        req_body = json.dumps({\n",
    "            \"add\": {\n",
    "                \"doc\": {\n",
    "                    \"id\": doc_id,\n",
    "                    \"year\": year,\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"text\": text,\n",
    "                    \"authors\": authors,\n",
    "                    \"keywords\": keywords\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        params = { \"commit\": \"true\" if should_commit else \"false\" }\n",
    "        requests.post(solr_url + \"update\", data=req_body, params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records added\n",
      "1000 records added\n",
      "2000 records added\n",
      "3000 records added\n",
      "4000 records added\n",
      "5000 records added\n",
      "6000 records added\n",
      "7000 records added\n",
      "7238 records added, COMPLETE\n",
      "7238 records in index\n"
     ]
    }
   ],
   "source": [
    "num_rows_in_index = count_index_rows(SOLR_URL)\n",
    "num_added = 0\n",
    "should_commit = False\n",
    "if num_rows_in_index == 0:\n",
    "    for textfile in os.listdir(TEXTFILES_DIR):\n",
    "        doc_id = int(textfile.split(\".\")[0])\n",
    "        if num_added % 1000 == 0:\n",
    "            print(\"{:d} records added\".format(num_added))\n",
    "            should_commit = True\n",
    "        add_row(SOLR_URL, doc_id, conn, valid_keywords, raw2canonical, \n",
    "                id2metadata, should_commit)        \n",
    "        should_commit = False\n",
    "        num_added += 1\n",
    "    \n",
    "    print(\"{:d} records added, COMPLETE\".format(num_added))\n",
    "    add_row(SOLR_URL, None, conn, valid_keywords, raw2canonical, id2metadata, True)\n",
    "    num_rows_in_index = count_index_rows(SOLR_URL)\n",
    "\n",
    "print(\"{:d} records in index\".format(num_rows_in_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
