{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Word Embeddings - Word2Vec, Doc2Vec\n",
    "\n",
    "The intuition behind __Word2Vec__ is that the meaning of a word can be inferred from its neighbors. Thus if we train a (shallow) neural network with word pairs which are close together, we will end up with a network that can predict if word pairs \"belong\" with each other or not. However, we are not going to use the neural network after training! Instead, the goal is to learn the weights of the hidden layer, which are essentially the word vectors (aka word embeddings) that we’re trying to learn. One way to think of these embeddings is as features that describe the target word.\n",
    "\n",
    "We will use the preprocessed text that we built for [16-topic-modeling notebook](http://localhost:8888/notebooks/16-topic-modeling.ipynb) to train a Gensim word2vec model and then use it to create document vectors similar to what we did there. We then use these document vectors in the same we we used the doc-topic vectors in the topic modeling notebook.\n",
    "\n",
    "Code in this notebook is adapted from the blog post [Word2Vec tutorial](https://rare-technologies.com/word2vec-tutorial/) by Radim Řehůřek, creator of gensim, and by the post [Gensim word2vec tutorial - full working example](http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/) by Kavita Ganesan.\n",
    "\n",
    "Yet another option instead of creating vectors manually is to build a __Doc2Vec__ model, which allows us to find similar documents directly. More details in the blog post [Doc2Vec Tutorial](https://rare-technologies.com/doc2vec-tutorial/) also by gensim creator Radim Řehůřek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "PREPROC_TEXTS_FILE = os.path.join(DATA_DIR, \"textfiles_preproc.txt\")\n",
    "\n",
    "WORD2VEC_MODEL_FILE = os.path.join(MODEL_DIR, \"word2vec_model.gensim\")\n",
    "DOC2VEC_MODEL_FILE = os.path.join(MODEL_DIR, \"doc2vec_model.gensim\")\n",
    "\n",
    "DOC_SIMS = os.path.join(DATA_DIR, \"w2v_sims.npy\")\n",
    "DOC_LOOKUP = os.path.join(DATA_DIR, \"w2v_docid2corpus.pkl\")\n",
    "\n",
    "PAPERS_METADATA = os.path.join(DATA_DIR, \"papers_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 12:03:49,163 : INFO : loading Word2Vec object from ../models/word2vec_model.gensim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec model already generated, loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 12:03:49,442 : INFO : loading wv recursively from ../models/word2vec_model.gensim.wv.* with mmap=None\n",
      "2018-08-25 12:03:49,443 : INFO : loading vectors from ../models/word2vec_model.gensim.wv.vectors.npy with mmap=None\n",
      "2018-08-25 12:03:49,486 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-08-25 12:03:49,487 : INFO : loading vocabulary recursively from ../models/word2vec_model.gensim.vocabulary.* with mmap=None\n",
      "2018-08-25 12:03:49,488 : INFO : loading trainables recursively from ../models/word2vec_model.gensim.trainables.* with mmap=None\n",
      "2018-08-25 12:03:49,489 : INFO : loading syn1neg from ../models/word2vec_model.gensim.trainables.syn1neg.npy with mmap=None\n",
      "2018-08-25 12:03:49,534 : INFO : setting ignored attribute cum_table to None\n",
      "2018-08-25 12:03:49,535 : INFO : loaded ../models/word2vec_model.gensim\n"
     ]
    }
   ],
   "source": [
    "class MyDocIterator(object):\n",
    "    def __init__(self, pp_filename):\n",
    "        self.fpp = open(pp_filename, \"r\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in self.fpp:\n",
    "            try:\n",
    "                filename, text = line.strip().split(\"\\t\")\n",
    "                yield text.split(\" \")\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "                \n",
    "class MyRestartableDocIterator(object):\n",
    "    def __init__(self, pp_filename):\n",
    "        self.pp_filename = pp_filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(MyDocIterator(self.pp_filename))\n",
    "\n",
    "\n",
    "if os.path.exists(WORD2VEC_MODEL_FILE):\n",
    "    print(\"word2vec model already generated, loading\")\n",
    "    w2v_model = gensim.models.Word2Vec.load(WORD2VEC_MODEL_FILE)\n",
    "else:\n",
    "    docs = MyRestartableDocIterator(PREPROC_TEXTS_FILE)\n",
    "    w2v_model = gensim.models.Word2Vec(docs, size=150, window=5, min_count=2, \n",
    "                                   workers=4, iter=10)\n",
    "    w2v_model.save(WORD2VEC_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion\n",
    "\n",
    "We can use the `most_similar` method of the learned model to find queries that are close to our query terms to find possible synonyms for query expansion. Note that we also treated our keywords as single multi-word tokens, so we can use them as-is as shown in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 12:03:49,712 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('convolve', 0.6773777008056641),\n",
       " ('upsampling', 0.6635823249816895),\n",
       " ('convolutional_layer', 0.6612037420272827),\n",
       " ('dilate', 0.6306341886520386),\n",
       " ('stride', 0.6231109499931335),\n",
       " ('convolutional', 0.6216310858726501),\n",
       " ('feature_map', 0.6172328591346741),\n",
       " ('deeper', 0.612465500831604),\n",
       " ('pool', 0.5934486985206604),\n",
       " ('max_pool', 0.586410641670227)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"convolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('neural_net', 0.7221011519432068),\n",
       " ('artificial_neural_network', 0.6340944766998291),\n",
       " ('deep_neural_network', 0.626783013343811),\n",
       " ('feedforward_network', 0.6249794363975525),\n",
       " ('recurrent_network', 0.5822201371192932),\n",
       " ('network_architecture', 0.5816317200660706),\n",
       " ('layer_perceptron', 0.5749163627624512),\n",
       " ('recurrent_neural_network', 0.5740536451339722),\n",
       " ('neuralnetworks_us', 0.5685520172119141),\n",
       " ('layer_network', 0.5418899059295654)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"neural_network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "The weights of the learned model is the word embeddings we are interested in, can be accessed as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99617, 150)\n"
     ]
    }
   ],
   "source": [
    "E = w2v_model.wv.vectors\n",
    "print(E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embeddings (BoW)\n",
    "\n",
    "We can extrapolate from these vectors to create document vectors using the bag-of-words approach, that is, each document vector is just the average of all its word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19770882  0.18564166  1.0081561  -0.18493582 -0.71190245 -0.23386441\n",
      "  0.44864107  0.42183316  0.32042122  0.03524575  0.39742947  0.2186656\n",
      " -0.33583622 -0.92449786 -0.27372528 -0.09199027 -0.45183092  1.26830638\n",
      " -0.23831061  0.45956633 -0.28053251 -0.09720943  1.03525012 -0.75988733\n",
      "  0.31549739  0.07462404 -0.20503044 -1.0408117  -0.53094931 -0.63574317\n",
      "  0.54676002  0.85588561 -0.18846756 -0.25591124  0.03638115  0.23548628\n",
      " -0.13348883 -0.80841208  0.34865554  0.08029212 -0.37310332 -1.12760907\n",
      " -0.18952324  0.09589486  0.3396261   0.35413956 -0.52332797 -0.10075037\n",
      " -0.06295226 -0.37133758  0.45845741 -0.43592981 -0.17267235 -0.75191418\n",
      " -0.02310579  0.22621982  0.57415657 -0.00700087  0.21481432  0.57625283\n",
      "  0.05762654 -0.8824854  -0.04926448 -0.04621982  0.26995589  0.07962886\n",
      " -0.44957382  0.32853732 -0.34244629  0.06719177 -0.63850286 -0.44873272\n",
      " -0.08850067  0.04439371 -0.43870203 -0.4641712   0.27890945 -0.06352812\n",
      "  0.47025011 -0.13218641  0.15521489 -0.55059321 -0.00493889 -0.44373864\n",
      "  0.00143001 -0.00277058 -0.66735437  0.51189178 -0.28239583 -0.33966119\n",
      " -0.28486092  0.66153202  0.41084222  0.62667865 -0.35287288  0.08332745\n",
      " -0.10330348 -0.75408282 -0.79277033 -0.61603279  0.44879266 -0.257484\n",
      " -0.57486996  0.16972473  0.05018081  0.15326414 -0.26225947 -0.19737225\n",
      "  0.26365996  0.32456056 -0.35469406 -0.2731081  -0.73935463  0.52473593\n",
      " -0.51352073 -0.77980159 -0.40440463  0.34120424 -0.54118508  0.19855815\n",
      " -0.39921148  0.4627792   0.9595583   0.23542889  0.12253286 -0.21243459\n",
      "  0.61067754  0.53886615 -0.40961493  0.08582456 -1.08814859 -0.15564301\n",
      "  0.3072419   0.02947882  0.59624542 -0.38775429  0.14627406  0.0112831\n",
      " -0.4494345  -0.60600185  0.34770021  0.03051624  0.04519837  0.130009\n",
      "  0.11049038  1.23798742  1.01039949 -0.28430932 -0.31850721 -0.68094393]\n"
     ]
    }
   ],
   "source": [
    "def compute_bow_docvec(text, w2v_model):\n",
    "    doc_vec = np.zeros(w2v_model.wv.vectors.shape[1])\n",
    "    num_words = 0\n",
    "    for word in text.split(\" \"):\n",
    "        try:\n",
    "            doc_vec += w2v_model.wv[word]\n",
    "            num_words += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    doc_vec /= num_words\n",
    "    return doc_vec\n",
    "\n",
    "\n",
    "test_text = \"organization associative database application hisashi suzuki suguru arimoto\"\n",
    "print(compute_bow_docvec(test_text, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 preprocessed docs read\n",
      "1000 preprocessed docs read\n",
      "2000 preprocessed docs read\n",
      "3000 preprocessed docs read\n",
      "4000 preprocessed docs read\n",
      "5000 preprocessed docs read\n",
      "6000 preprocessed docs read\n",
      "7000 preprocessed docs read\n",
      "7238 preprocessed docs read, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "doc_vectors = []\n",
    "docid2corpus = {}\n",
    "row_id = 0\n",
    "fppt = open(PREPROC_TEXTS_FILE, \"r\")\n",
    "for line in fppt:\n",
    "    if row_id % 1000 == 0:\n",
    "        print(\"{:d} preprocessed docs read\".format(row_id))\n",
    "    try:\n",
    "        filename, text = line.strip().split(\"\\t\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    doc_vectors.append(compute_bow_docvec(text, w2v_model))\n",
    "    doc_id = int(filename.split(\".\")[0])\n",
    "    docid2corpus[doc_id] = row_id\n",
    "    row_id += 1\n",
    "fppt.close()\n",
    "print(\"{:d} preprocessed docs read, COMPLETE\".format(row_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7238, 150)\n"
     ]
    }
   ],
   "source": [
    "D = np.array(doc_vectors)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity (BoW)\n",
    "\n",
    "Finally we compute the cosine similarity between all pairs of documents by multiplying the document vector with a transpose of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7238, 7238)\n"
     ]
    }
   ],
   "source": [
    "sim = np.matmul(D, np.transpose(D)) / np.linalg.norm(D)\n",
    "print(sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DOC_SIMS, sim)\n",
    "pickle.dump(docid2corpus, open(DOC_LOOKUP, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Forward-backward retraining of recurrent neural networks\n",
      "--- top 10 similar docs ---\n",
      "(0.06981) Forward-backward retraining of recurrent neural networks\n",
      "(0.06857) Better Generative Models for Sequential Data Problems: Bidirectional Recurrent Mixture Density Networks\n",
      "(0.06831) A Recurrent Neural Network for Word Identification from Continuous Phoneme Strings\n",
      "(0.06652) Learning Sequential Structure in Simple Recurrent Networks\n",
      "(0.06582) Speech Recognition Using Demi-Syllable Neural Prediction Model\n",
      "(0.06512) Searching for Character Models\n",
      "(0.06498) Speech Production Using A Neural Network with a Cooperative Learning Mechanism\n",
      "(0.06488) An Integrated Architecture of Adaptive Neural Network Control for Dynamic Systems\n",
      "(0.06454) HMM Speech Recognition with Neural Net Discrimination\n",
      "(0.06425) A Segment-Based Automatic Language Identification System\n"
     ]
    }
   ],
   "source": [
    "def similar_docs(filename, sim, topn, docid2corpus, corpusid2doc):\n",
    "    doc_id = int(filename.split(\".\")[0])\n",
    "    corpus_id = docid2corpus[doc_id]\n",
    "    row = sim[corpus_id, :]\n",
    "    target_docs = np.argsort(-row)[0:topn].tolist()\n",
    "    scores = row[target_docs].tolist()\n",
    "    target_filenames = [\"{:d}.txt\".format(corpusid2doc[x]) for x in target_docs]\n",
    "    return target_filenames, scores\n",
    "    \n",
    "\n",
    "filename2title = {}\n",
    "with open(PAPERS_METADATA, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        cols = line.strip().split(\"\\t\")\n",
    "        filename2title[\"{:s}.txt\".format(cols[0])] = cols[2]\n",
    "\n",
    "source_filename = \"1032.txt\"\n",
    "top_n = 10\n",
    "corpusid2doc = {v:k for k, v in docid2corpus.items()}\n",
    "target_filenames, scores = similar_docs(source_filename, sim, top_n, \n",
    "                                        docid2corpus, corpusid2doc)\n",
    "print(\"Source: {:s}\".format(filename2title[source_filename]))\n",
    "print(\"--- top {:d} similar docs ---\".format(top_n))\n",
    "for target_filename, score in zip(target_filenames, scores):\n",
    "    print(\"({:.5f}) {:s}\".format(score, filename2title[target_filename]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity with Doc2Vec\n",
    "\n",
    "We could dispense with having to create our own similarity matrix and just use a model such as Doc2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 12:04:27,149 : INFO : loading Doc2Vec object from ../models/doc2vec_model.gensim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2vec model file exists, loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 12:04:28,002 : INFO : loading vocabulary recursively from ../models/doc2vec_model.gensim.vocabulary.* with mmap=None\n",
      "2018-08-25 12:04:28,003 : INFO : loading trainables recursively from ../models/doc2vec_model.gensim.trainables.* with mmap=None\n",
      "2018-08-25 12:04:28,004 : INFO : loading syn1neg from ../models/doc2vec_model.gensim.trainables.syn1neg.npy with mmap=None\n",
      "2018-08-25 12:04:28,129 : INFO : loading wv recursively from ../models/doc2vec_model.gensim.wv.* with mmap=None\n",
      "2018-08-25 12:04:28,130 : INFO : loading vectors from ../models/doc2vec_model.gensim.wv.vectors.npy with mmap=None\n",
      "2018-08-25 12:04:28,261 : INFO : loading docvecs recursively from ../models/doc2vec_model.gensim.docvecs.* with mmap=None\n",
      "2018-08-25 12:04:28,262 : INFO : loaded ../models/doc2vec_model.gensim\n"
     ]
    }
   ],
   "source": [
    "class TaggedDocIterator(object):\n",
    "    def __init__(self, pp_filename):\n",
    "        self.fpp = open(pp_filename, \"r\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in self.fpp:\n",
    "            try:\n",
    "                filename, text = line.strip().split(\"\\t\")\n",
    "                doc_id = int(filename.split(\".\")[0])\n",
    "                yield gensim.models.doc2vec.TaggedDocument(\n",
    "                    words=text.split(\" \"), tags=[doc_id])\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "\n",
    "class RestartableTaggedDocIterator(object):\n",
    "    def __init__(self, pp_filename):\n",
    "        self.pp_filename = pp_filename\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return iter(TaggedDocIterator(self.pp_filename))\n",
    "\n",
    "\n",
    "if os.path.exists(DOC2VEC_MODEL_FILE):\n",
    "    print(\"doc2vec model file exists, loading\")\n",
    "    d2v_model = gensim.models.doc2vec.Doc2Vec.load(DOC2VEC_MODEL_FILE)\n",
    "else:\n",
    "    docs = RestartableTaggedDocIterator(PREPROC_TEXTS_FILE)            \n",
    "    d2v_model = gensim.models.doc2vec.Doc2Vec(docs, vector_size=150, window=5, \n",
    "                                              min_count=1, workers=4, iter=10)\n",
    "    d2v_model.save(DOC2VEC_MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 12:04:28,781 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(427, 0.6177938580513),\n",
       " (1347, 0.59282386302948),\n",
       " (38, 0.5886051654815674),\n",
       " (269, 0.5721673965454102),\n",
       " (88, 0.5622702240943909),\n",
       " (125, 0.5453654527664185),\n",
       " (1569, 0.5206254720687866),\n",
       " (839, 0.5154223442077637),\n",
       " (5872, 0.5119458436965942),\n",
       " (211, 0.5094888210296631)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.most_similar(positive=[1], negative=[], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Forward-backward retraining of recurrent neural networks\n",
      "--- top 10 similar docs ---\n",
      "(0.67334) Phonetic Classification and Recognition Using the Multi-Layer Perceptron\n",
      "(0.65860) A Continuous Speech Recognition System Embedding MLP into HMM\n",
      "(0.64828) Connectionist Approaches to the Use of Markov Models for Speech Recognition\n",
      "(0.63736) REMAP: Recursive Estimation and Maximization of A Posteriori Probabilities - Application to Transition-Based Connectionist Speech Recognition\n",
      "(0.63566) Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System\n",
      "(0.62960) Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks\n",
      "(0.61769) Modeling Consistency in a Speaker Independent Continuous Speech Recognition System\n",
      "(0.61722) Improved Hidden Markov Model Speech Recognition Using Radial Basis Function Networks\n",
      "(0.61304) Handwritten Word Recognition using Contextual Hybrid Radial Basis Function Network/Hidden Markov Models\n",
      "(0.61246) English Alphabet Recognition with Telephone Speech\n"
     ]
    }
   ],
   "source": [
    "source_filename = \"1032.txt\"\n",
    "top_n = 10\n",
    "\n",
    "print(\"Source: {:s}\".format(filename2title[source_filename]))\n",
    "print(\"--- top {:d} similar docs ---\".format(top_n))\n",
    "\n",
    "source_docid = int(source_filename.split(\".\")[0])\n",
    "targetid_scores = d2v_model.docvecs.most_similar(positive=[source_docid], \n",
    "                                                 negative=[], topn=top_n)\n",
    "\n",
    "for target_id, score in targetid_scores:\n",
    "    target_filename = \"{:d}.txt\".format(target_id)\n",
    "    print(\"({:.5f}) {:s}\".format(score, filename2title[target_filename]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
