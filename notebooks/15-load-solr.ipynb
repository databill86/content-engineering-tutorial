{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Solr with keyword facets\n",
    "\n",
    "In our previous Solr index, had added features to do the following:\n",
    "\n",
    "* Show facets along with search results to allow user to drill down into the subset of search results given by the facet.\n",
    "* Allow better query parsing by using the keywords as a dictionary which can be looked up to isolate longer entities than just words.\n",
    "\n",
    "In this instance, we will do the following:\n",
    "\n",
    "* Add our newly mined ORG keywords as facets.\n",
    "\n",
    "In addition, we will now focus on the content discovery side, adding features to do the following:\n",
    "\n",
    "* Implement More Like This using title, abstract and text. This requires thee fields to have termVectors turned on.\n",
    "* Implement Similar Documents functionality using keywords, authors and ORGs (similar to More Like This, except implementation reuses the facets we have already built).\n",
    "* A more focused More Like This that uses a combined field kaoterms which stores multi-word terms as tokens. Internally MLT will use this field instead of title, abstract and text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "WORDCOUNTS_DB = os.path.join(DATA_DIR, \"wordcounts.db\")\n",
    "\n",
    "CURATED_KEYWORDS = os.path.join(DATA_DIR, \"raw_keywords.txt\")\n",
    "NEARDUP_MAPPINGS = os.path.join(DATA_DIR, \"keyword_neardup_mappings.tsv\")\n",
    "DEDUPE_MAPPINGS = os.path.join(DATA_DIR, \"keyword_dedupe_mappings.tsv\")\n",
    "\n",
    "TEXTFILES_DIR = os.path.join(DATA_DIR, \"textfiles\")\n",
    "METADATA_FILE = os.path.join(DATA_DIR, \"papers_metadata.tsv\")\n",
    "\n",
    "ORGS_DIR = os.path.join(DATA_DIR, \"orgs\")\n",
    "\n",
    "SOLR_URL = \"http://localhost:8983/solr/nips2index/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(WORDCOUNTS_DB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index with keyword facets\n",
    "\n",
    "First we create a new core to hold our new index.\n",
    "\n",
    "    cd <solr_home>\n",
    "    bin/solr create -c nips2index\n",
    "    \n",
    "Then add in our new schema. This has one additional multiValued string field keywords to hold our facets. Additionally we have changed the type of the authors field to \"string\" to allow faceting on authors as well.\n",
    "\n",
    "    cd ../scripts\n",
    "    ./create_config2.sh   # add MLT component\n",
    "    ./create_schema2.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup tables for keywords\n",
    "\n",
    "The full list of keywords is available in the `bigrams`, `rake` and `maui` tables. But they need to be filtered using the manually curated list of keywords. Finally these keywords need to be replaced with their canonical form so they look nicer when displayed as facets. \n",
    "\n",
    "We will also do a reverse lookup on these canonical form lookup tables when working on query expansion later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2282 valid keywords\n"
     ]
    }
   ],
   "source": [
    "valid_keywords = set()\n",
    "fcurated = open(CURATED_KEYWORDS, \"r\")\n",
    "for line in fcurated:\n",
    "    valid_keywords.add(line.strip())\n",
    "fcurated.close()\n",
    "\n",
    "print(\"{:d} valid keywords\".format(len(valid_keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 raw to canonical mappings\n"
     ]
    }
   ],
   "source": [
    "raw2canonical = {}\n",
    "fneardup = open(NEARDUP_MAPPINGS, \"r\")\n",
    "for line in fneardup:\n",
    "    key, value = line.strip().split(\"\\t\")\n",
    "    raw2canonical[key] = value\n",
    "fneardup.close()\n",
    "\n",
    "fdedupe = open(DEDUPE_MAPPINGS, \"r\")\n",
    "for line in fdedupe:\n",
    "    key, value, _ = line.strip().split(\"\\t\")\n",
    "    raw2canonical[key] = value\n",
    "fdedupe.close()\n",
    "\n",
    "print(\"{:d} raw to canonical mappings\".format(len(raw2canonical)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total number', 'lower bound', 'ieee transactions', 'neural network', 'science foundation', 'hamming distance', 'neural net', 'upper bound', 'asymptotically optimal', 'maximum number', 'internal representation', 'matrix multiplication', 'optimal performance', 'information theory', 'random variables', 'computer science', 'national science foundation', 'associative memory', 'optimal values', 'national science']\n"
     ]
    }
   ],
   "source": [
    "def get_keywords_for_doc(conn, doc_id, valid_keywords, raw2canonical):\n",
    "    keywords = set()\n",
    "    # collect from bigrams\n",
    "    cur_bigrams = conn.cursor()\n",
    "    cur_bigrams.execute(\"select word_1, word_2 from bigrams where doc_id = ?\", [doc_id])\n",
    "    rows = cur_bigrams.fetchall()\n",
    "    for row in rows:\n",
    "        keyword = \" \".join([row[0], row[1]]).lower()\n",
    "        keywords.add(keyword)\n",
    "    cur_bigrams.close()\n",
    "    # collect from rake\n",
    "    cur_rake = conn.cursor()\n",
    "    cur_rake.execute(\"select keyword from rake where doc_id = ?\", [doc_id])\n",
    "    rows = cur_rake.fetchall()\n",
    "    for row in rows:\n",
    "        keywords.add(row[0].lower())\n",
    "    cur_rake.close()\n",
    "    # collect from maui\n",
    "    cur_maui = conn.cursor()\n",
    "    cur_maui.execute(\"select keyword from maui where doc_id = ?\", [doc_id])\n",
    "    rows = cur_maui.fetchall()\n",
    "    for row in rows:\n",
    "        keywords.add(row[0].lower())\n",
    "    cur_maui.close()\n",
    "    # filter out valid keywords\n",
    "    filtered_keywords = [keyword for keyword in keywords \n",
    "                         if keyword in valid_keywords]\n",
    "    # map them to their canonical forms if applicable\n",
    "    canonical_keywords = set()\n",
    "    for keyword in filtered_keywords:\n",
    "        if keyword in raw2canonical.keys():\n",
    "            canonical_keywords.add(raw2canonical[keyword])\n",
    "        else:\n",
    "            canonical_keywords.add(keyword)\n",
    "    # return as list\n",
    "    return list(canonical_keywords)\n",
    "\n",
    "\n",
    "doc_id = 1\n",
    "print(get_keywords_for_doc(conn, doc_id, valid_keywords, raw2canonical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document metadata lookup\n",
    "\n",
    "We already saved the non-text values for each document in the `papers_metadata.tsv` file in the `01-preprocess` notebook, so we will use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7238 metadata mappings\n"
     ]
    }
   ],
   "source": [
    "id2metadata = {}\n",
    "fmeta = open(METADATA_FILE, \"r\")\n",
    "for line in fmeta:\n",
    "    line = line.strip()\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "    id, year, title, abstract, author_names = line.split(\"\\t\")\n",
    "    authors = author_names.split(\":\")\n",
    "    id2metadata[int(id)] = (year, title, abstract, authors)\n",
    "\n",
    "fmeta.close()\n",
    "print(\"{:d} metadata mappings\".format(len(id2metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORG lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['University of Toronto', 'Aston University']\n"
     ]
    }
   ],
   "source": [
    "def get_orgs_for_doc(doc_id):\n",
    "    orgs = []\n",
    "    forg = open(os.path.join(ORGS_DIR, \"{:d}.org\".format(doc_id)), \"r\")\n",
    "    for line in forg:\n",
    "        orgs.append(line.strip())\n",
    "    forg.close()\n",
    "    return orgs\n",
    "\n",
    "\n",
    "print(get_orgs_for_doc(978))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAO Terms builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural0network fitting0models control0points Suguru0Arimoto Phillip0A.0Chou Osaka0UniversityCalTech\n"
     ]
    }
   ],
   "source": [
    "def build_kao_field(keywords, authors, orgs):\n",
    "    kao_terms = []\n",
    "    for multi_word_term in keywords + authors + orgs:\n",
    "        kao_terms.append(multi_word_term.replace(\" \", \"0\"))\n",
    "    return \" \".join(kao_terms)\n",
    "\n",
    "print(build_kao_field([\"neural network\", \"fitting models\", \"control points\"],\n",
    "                      [\"Suguru Arimoto\", \"Phillip A. Chou\"],\n",
    "                      [\"Osaka University\" \"CalTech\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load papers into Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_index_rows(solr_url):\n",
    "    resp = requests.get(solr_url + \"select?q=*:*\")\n",
    "    resp_json = json.loads(resp.text)\n",
    "    return resp_json[\"response\"][\"numFound\"]\n",
    "\n",
    "\n",
    "def add_row(solr_url, doc_id, conn, valid_keywords, raw2canonical, \n",
    "            id2metadata, should_commit):\n",
    "    headers = {\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"accept\": \"application/json\"\n",
    "    }\n",
    "    if doc_id is None:\n",
    "        requests.post(solr_url + \"update\", params={\"commit\":\"true\"}, headers=headers)\n",
    "    else:\n",
    "        ftext = open(os.path.join(TEXTFILES_DIR, \"{:d}.txt\".format(doc_id)), \"r\")\n",
    "        textfile_lines = []\n",
    "        for line in ftext:\n",
    "            textfile_lines.append(line.strip())\n",
    "        ftext.close()\n",
    "        text = \"\\n\".join(textfile_lines)\n",
    "        year, title, abstract, authors = id2metadata[doc_id]\n",
    "        keywords = get_keywords_for_doc(conn, doc_id, valid_keywords, raw2canonical)\n",
    "        orgs = get_orgs_for_doc(doc_id)\n",
    "        kaoterms = build_kao_field(keywords, authors, orgs)\n",
    "        req_body = json.dumps({\n",
    "            \"add\": {\n",
    "                \"doc\": {\n",
    "                    \"id\": doc_id,\n",
    "                    \"year\": year,\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"text\": text,\n",
    "                    \"authors\": authors,\n",
    "                    \"keywords\": keywords,\n",
    "                    \"orgs\": orgs,\n",
    "                    \"kaoterms\": kaoterms\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        params = { \"commit\": \"true\" if should_commit else \"false\" }\n",
    "        requests.post(solr_url + \"update\", data=req_body, params=params, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records added\n",
      "1000 records added\n",
      "2000 records added\n",
      "3000 records added\n",
      "4000 records added\n",
      "5000 records added\n",
      "6000 records added\n",
      "7000 records added\n",
      "7238 records added, COMPLETE\n",
      "7238 records in index\n"
     ]
    }
   ],
   "source": [
    "num_rows_in_index = count_index_rows(SOLR_URL)\n",
    "num_added = 0\n",
    "should_commit = False\n",
    "if num_rows_in_index == 0:\n",
    "    for textfile in os.listdir(TEXTFILES_DIR):\n",
    "        doc_id = int(textfile.split(\".\")[0])\n",
    "        if num_added % 1000 == 0:\n",
    "            print(\"{:d} records added\".format(num_added))\n",
    "            should_commit = True\n",
    "        add_row(SOLR_URL, doc_id, conn, valid_keywords, raw2canonical, \n",
    "                id2metadata, should_commit)        \n",
    "        should_commit = False\n",
    "        num_added += 1\n",
    "    \n",
    "    print(\"{:d} records added, COMPLETE\".format(num_added))\n",
    "    add_row(SOLR_URL, None, conn, valid_keywords, raw2canonical, id2metadata, True)\n",
    "    num_rows_in_index = count_index_rows(SOLR_URL)\n",
    "\n",
    "print(\"{:d} records in index\".format(num_rows_in_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
