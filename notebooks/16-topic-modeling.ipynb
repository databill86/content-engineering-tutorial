{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "A Topic Modeling algorithm ends up with each topic consisting of a collection of terms in different proportions given by P(topic|term), and each document is a collection of topics in different proportions given by P(doc|topic). Topic modeling is an unsupervised algorithm. There are various algorithms available in gensim, as listed below:\n",
    "\n",
    "* LSI (Latent Semantic Indexing)\n",
    "* HDP (Hierarchical Dirichlet Process)\n",
    "* LDA (Latent Dirichlet Allocation)\n",
    "* Mallet wrapper\n",
    "\n",
    "Of these, HDP can work out the optimum number of topics, so we use that here.\n",
    "\n",
    "We use topic modeling as a dimensionality reduction technique, whereby each document moves from being a vector of tokens (vocabulary words and n-grams) to being a smaller, hopefully denser, vector of topics. Collection of document topic vectors is a matrix, which can be used to generate a document-document similarity matrix.\n",
    "\n",
    "Topic Modeling code is adapted from this [gensim example notebook](https://markroxor.github.io/gensim/static/notebooks/gensim_news_classification.html), as well as the [HDP Documentation Page](https://radimrehurek.com/gensim/models/hdpmodel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ahocorasick\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "MODEL_DIR = \"../models\"\n",
    "\n",
    "CURATED_KEYWORDS = os.path.join(DATA_DIR, \"raw_keywords.txt\")\n",
    "KEYWORD_MAPPING_FILES = [\n",
    "    os.path.join(DATA_DIR, \"keyword_neardup_mappings.tsv\"),\n",
    "    os.path.join(DATA_DIR, \"keyword_dedupe_mappings.tsv\")\n",
    "]\n",
    "\n",
    "CUSTOM_STOPWORDS = os.path.join(DATA_DIR, \"stopwords.txt\")\n",
    "\n",
    "TEXTFILES_DIR = os.path.join(DATA_DIR, \"textfiles\")\n",
    "TEXTFILES_PREPROC = os.path.join(DATA_DIR, \"textfiles_preproc.txt\")\n",
    "\n",
    "HDP_MODEL = os.path.join(MODEL_DIR, \"hdp_model.gensim\")\n",
    "\n",
    "TOPIC_SIMS = os.path.join(DATA_DIR, \"topic_sims.npy\")\n",
    "TOPIC_LOOKUP = os.path.join(DATA_DIR, \"topic_docid2corpus.pkl\")\n",
    "\n",
    "PAPERS_METADATA = os.path.join(DATA_DIR, \"papers_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We apply the following three preprocessing steps to our data.\n",
    "* Collocation Detection\n",
    "* Lemmatization\n",
    "* Stopword Removal\n",
    "\n",
    "### Collocation Detection\n",
    "\n",
    "We will load up a trie with our curated keywords, and then run through each text file replacing the multi-word token into a single one by replacing space chars with underscore. Keywords that refer to the same thing will be collapsed using the mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built dictionary trie with 2282 entries\n"
     ]
    }
   ],
   "source": [
    "keywords_dict = ahocorasick.Automaton()\n",
    "fkeys = open(CURATED_KEYWORDS, \"r\")\n",
    "for idx, keyword in enumerate(fkeys):\n",
    "    keyword = keyword.strip()\n",
    "    keywords_dict.add_word(keyword, (idx, keyword))\n",
    "fkeys.close()\n",
    "keywords_dict.make_automaton()\n",
    "\n",
    "print(\"built dictionary trie with {:d} entries\".format(len(keywords_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455 canonical keyword mappings\n"
     ]
    }
   ],
   "source": [
    "canonical_keywords = {}\n",
    "for keyword_mapping_file in KEYWORD_MAPPING_FILES:\n",
    "    fmap = open(keyword_mapping_file, \"r\")\n",
    "    for line in fmap:\n",
    "        cols = line.strip().split(\"\\t\")\n",
    "        canonical_keywords[cols[0]] = cols[1]\n",
    "    fmap.close()\n",
    "    \n",
    "print(\"{:d} canonical keyword mappings\".format(len(canonical_keywords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on these results it is possible to compare the MEM to other families of models (e.g., neural networks and state dependent models). It is shown that a degenerate version of the MEM is in fact equivalent to a neural network, and the number of experts in the architecture plays a similar role to the number of hidden units in the latter model.\n",
      "---\n",
      "Based on these results it is possible to compare the MEM to other families of models (e.g., neural_network and state dependent models). It is shown that a degenerate version of the MEM is in fact equivalent to a neural_network, and the number of experts in the architecture plays a similar role to the number of hidden_unit in the latter model.\n"
     ]
    }
   ],
   "source": [
    "def find_and_replace(text, keywords_dict, canonical_keywords):\n",
    "    # find\n",
    "    matches = []\n",
    "    for end_index, (insert_order, keyword) in keywords_dict.iter(text):\n",
    "        start_index = end_index - len(keyword) + 1\n",
    "        matches.append((start_index, end_index + 1, keyword))\n",
    "    # replace\n",
    "    text_chars = list(text)\n",
    "    for start, end, source in matches:\n",
    "        if source in canonical_keywords.keys():\n",
    "            can_source = canonical_keywords[source]\n",
    "            target = can_source.replace(\" \", \"_\")\n",
    "        else:\n",
    "            target = source.replace(\" \", \"_\")\n",
    "        target = target.rjust(len(source))\n",
    "        target_chars = list(target)\n",
    "        j = 0\n",
    "        for i in range(start, end):\n",
    "            text_chars[i] = target_chars[j]\n",
    "            j += 1\n",
    "    # return\n",
    "    return re.sub(\"\\s+\", \" \", \"\".join(text_chars))\n",
    "\n",
    "\n",
    "text = \"\"\"Based on these results it is possible to compare the MEM to other families \n",
    "of models (e.g., neural networks and state dependent models). It is shown that a degenerate \n",
    "version of the MEM is in fact equivalent to a neural network, and the number of experts \n",
    "in the architecture plays a similar role to the number of hidden units in\n",
    "the latter model.\"\"\"\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "text = re.sub(\"\\s+\", \" \", text)\n",
    "print(text)\n",
    "print(\"---\")\n",
    "text_with_collocs = find_and_replace(text, keywords_dict, canonical_keywords)\n",
    "print(text_with_collocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Combine SpaCy default, NLTK default and custom stopwords for corpus. We will use SpaCy as our NLP toolkit, so everything merged into SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-stopwords from SpaCy: 305\n",
      "#-stopwords including custom: 579\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print(\"#-stopwords from SpaCy:\", len(STOP_WORDS))\n",
    "\n",
    "# add NLTK stopwords\n",
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "custom_stopwords = []\n",
    "custom_stopwords.extend(nltk_stopwords)\n",
    "\n",
    "# add our corpus specific custom stopwords\n",
    "fstops = open(CUSTOM_STOPWORDS, \"r\")\n",
    "for stopword in fstops:\n",
    "    custom_stopwords.append(stopword.strip())\n",
    "fstops.close()\n",
    "\n",
    "# add punctuation\n",
    "punct_chars = [c for c in string.punctuation]\n",
    "for punct_char in punct_chars:\n",
    "    custom_stopwords.append(punct_char)\n",
    "\n",
    "for stopword in custom_stopwords:\n",
    "    STOP_WORDS.add(stopword)\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True\n",
    "    \n",
    "print(\"#-stopwords including custom:\", len(STOP_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compare', 'family', 'neural_network', 'dependent', 'degenerate', 'version', 'equivalent', 'neural_network', 'expert', 'architecture', 'hidden_unit']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_and_remove_stopwords(text, nlp, stopwords):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        if token.like_num:\n",
    "            continue\n",
    "        if len(token.text) <= 5:\n",
    "            continue\n",
    "        if token.text.startswith(\"-\") and token.text.endswith(\"-\"):\n",
    "            # -PRON-, etc\n",
    "            continue\n",
    "        lemma = token.lemma_\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "print(lemmatize_and_remove_stopwords(text_with_collocs, nlp, STOP_WORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Corpus for Topic Modeling\n",
    "\n",
    "We apply the pipeline to all our text files, then create the dictionary and corpus objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 preprocessed texts read\n",
      "1000 preprocessed texts read\n",
      "2000 preprocessed texts read\n",
      "3000 preprocessed texts read\n",
      "4000 preprocessed texts read\n",
      "5000 preprocessed texts read\n",
      "6000 preprocessed texts read\n",
      "7000 preprocessed texts read\n",
      "7238 preprocessed texts read, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "docid2corpus, corpusid2doc = {}, {}\n",
    "train_texts = []\n",
    "corpus_id = 0\n",
    "    \n",
    "if os.path.exists(TEXTFILES_PREPROC):\n",
    "    fprep = open(TEXTFILES_PREPROC, \"r\")\n",
    "    for line in fprep:\n",
    "        if corpus_id % 1000 == 0:\n",
    "            print(\"{:d} preprocessed texts read\".format(corpus_id))\n",
    "        try:\n",
    "            filename, text = line.strip().split(\"\\t\")\n",
    "            doc_id = int(filename.split(\".\")[0])\n",
    "            tokens = text.split(\" \")\n",
    "            docid2corpus[doc_id] = corpus_id\n",
    "            corpusid2doc[corpus_id] = doc_id\n",
    "            train_texts.append(tokens)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        corpus_id += 1\n",
    "    fprep.close()\n",
    "    print(\"{:d} preprocessed texts read, COMPLETE\".format(corpus_id))\n",
    "else:\n",
    "    fprep = open(TEXTFILES_PREPROC, \"w\")\n",
    "    for textfile in os.listdir(TEXTFILES_DIR):\n",
    "        if corpus_id % 100 == 0:\n",
    "            print(\"{:d} files read\".format(corpus_id))\n",
    "        doc_id = int(textfile.split(\".\")[0])\n",
    "        docid2corpus[doc_id] = corpus_id\n",
    "        corpusid2doc[corpus_id] = doc_id\n",
    "        ftext = open(os.path.join(TEXTFILES_DIR, textfile), \"r\")\n",
    "        lines = []\n",
    "        for line in ftext:\n",
    "            lines.append(line.strip())\n",
    "        ftext.close()\n",
    "        text = \" \".join(lines)\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = re.sub(\"\\s+\", \" \", text)\n",
    "        text = find_and_replace(text, keywords_dict, canonical_keywords)\n",
    "        text = lemmatize_and_remove_stopwords(text, nlp, STOP_WORDS)\n",
    "        train_texts.append(text)\n",
    "        fprep.write(\"{:s}\\t{:s}\\n\".format(textfile, \" \".join(text)))\n",
    "        corpus_id += 1\n",
    "    print(\"{:d} files read, COMPLETE\".format(corpus_id))\n",
    "    fprep.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-documents in train_texts: 7235\n",
      "7235 rows, 3 to 1645 cols each\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(train_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "\n",
    "print(\"#-documents in train_texts: {:d}\".format(len(train_texts)))\n",
    "print(\"{:d} rows, {:d} to {:d} cols each\".format(\n",
    "    len(corpus), \n",
    "    min([len(row) for row in corpus]),\n",
    "    max([len(row) for row in corpus])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build topic model\n",
    "\n",
    "We use the HDP model because we don't know the correct number of topics for our corpus. Once we have a rough idea (either from a preliminary run of HDP or knowledge about the corpus), we can use [Topic Coherence](https://rare-technologies.com/what-is-topic-coherence/) or [Perplexity](https://en.wikipedia.org/wiki/Perplexity) to decide the best algorithm and/or the best number of topics. In our case, our intent is not to display the topics, but rather to use the topics as features for our similarity calculations, so we don't spend too much time on that and just use the results of HDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*figure + 0.004*feature + 0.003*sample + 0.003*theorem + 0.003*weight + 0.003*kernel + 0.003*gaussian + 0.003*dataset + 0.002*learning + 0.002*neural'),\n",
       " (1,\n",
       "  '0.006*figure + 0.004*gaussian + 0.004*kernel + 0.003*weight + 0.003*neural + 0.003*pattern + 0.003*neuron + 0.003*signal + 0.003*system + 0.003*equation'),\n",
       " (2,\n",
       "  '0.005*figure + 0.003*feature + 0.003*weight + 0.003*represent + 0.003*gaussian + 0.003*theorem + 0.003*action + 0.003*neural + 0.003*equation + 0.002*neuron'),\n",
       " (3,\n",
       "  '0.007*action + 0.005*figure + 0.005*system + 0.005*policy + 0.004*pattern + 0.004*control + 0.003*weight + 0.003*state + 0.003*channel + 0.003*context'),\n",
       " (4,\n",
       "  '0.005*figure + 0.004*gaussian + 0.004*weight + 0.003*represent + 0.003*neural + 0.002*classifier + 0.002*control + 0.002*representation + 0.002*learn + 0.002*learning'),\n",
       " (5,\n",
       "  '0.004*figure + 0.003*system + 0.003*weight + 0.003*equation + 0.003*gradient + 0.003*neural + 0.003*sequence + 0.003*gaussian + 0.002*dimension + 0.002*bound'),\n",
       " (6,\n",
       "  '0.005*figure + 0.005*neuron + 0.004*weight + 0.004*pattern + 0.004*cluster + 0.003*system + 0.003*sequence + 0.003*memory + 0.003*gaussian + 0.003*connection'),\n",
       " (7,\n",
       "  '0.004*correlation + 0.003*frequency + 0.003*control + 0.003*cortical + 0.003*pattern + 0.003*student + 0.003*teacher + 0.003*activity + 0.002*current + 0.002*equation'),\n",
       " (8,\n",
       "  '0.003*instruction + 0.003*entropy + 0.003*change + 0.003*schedule + 0.002*decision + 0.002*committee + 0.002*design + 0.002*procedure + 0.002*policy + 0.002*temperature'),\n",
       " (9,\n",
       "  '0.004*system + 0.002*channel + 0.002*frequency + 0.002*target + 0.002*expert + 0.002*stream + 0.002*gradient + 0.002*vector + 0.002*preference + 0.002*learn'),\n",
       " (10,\n",
       "  '0.004*figure + 0.003*signal + 0.002*hidden_unit + 0.002*period + 0.002*utility + 0.002*circuit + 0.002*automaton + 0.002*system + 0.002*equation + 0.002*represent'),\n",
       " (11,\n",
       "  '0.004*neuron + 0.003*source + 0.003*pattern + 0.003*capacity + 0.002*retrieval + 0.002*feature + 0.002*memory + 0.002*neural_network + 0.002*address + 0.002*neural'),\n",
       " (12,\n",
       "  '0.004*circuit + 0.004*ensemble + 0.003*student + 0.003*figure + 0.003*character + 0.003*response + 0.002*gaussian + 0.002*weight + 0.002*retina + 0.002*intensity'),\n",
       " (13,\n",
       "  '0.003*release + 0.003*visual + 0.002*synapse + 0.002*cortical + 0.002*temporal + 0.002*pattern + 0.002*response + 0.002*dynamic + 0.002*stimulus + 0.002*synapsis'),\n",
       " (14,\n",
       "  '0.003*neuron + 0.002*pattern + 0.002*stimulus + 0.002*stability + 0.002*eigenvalue + 0.002*response + 0.002*figure + 0.001*signal + 0.001*center + 0.001*condition'),\n",
       " (15,\n",
       "  '0.004*weight + 0.004*cluster + 0.002*sample + 0.002*similarity + 0.002*expert + 0.002*mixture + 0.002*data_set + 0.002*clustering + 0.002*class + 0.001*active'),\n",
       " (16,\n",
       "  '0.009*expert + 0.003*figure + 0.002*example + 0.002*receptive_field + 0.001*ensemble + 0.001*prediction + 0.001*feature + 0.001*boost + 0.001*weight + 0.001*weak_learner'),\n",
       " (17,\n",
       "  '0.003*region + 0.003*generalize + 0.003*figure + 0.002*committee + 0.002*signal + 0.002*dimension + 0.002*example + 0.002*error_function + 0.002*neural + 0.002*local_minima'),\n",
       " (18,\n",
       "  '0.003*facial + 0.003*action + 0.002*classifier + 0.002*figure + 0.002*label + 0.002*attribute + 0.002*apobayesian + 0.001*classify + 0.001*image + 0.001*mistake'),\n",
       " (19,\n",
       "  '0.004*rout + 0.003*signal + 0.002*representation + 0.002*emotion + 0.002*traffic + 0.002*analog + 0.002*pattern + 0.002*neural_network + 0.002*source + 0.002*feature')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(HDP_MODEL):\n",
    "    # load model\n",
    "    hdpmodel = gensim.models.HdpModel.load(HDP_MODEL)\n",
    "else:\n",
    "    # train model and save\n",
    "    hdpmodel = gensim.models.HdpModel(corpus=corpus, id2word=dictionary)\n",
    "    hdpmodel.save(HDP_MODEL)\n",
    "\n",
    "hdpmodel.print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 305037)\n"
     ]
    }
   ],
   "source": [
    "topic_terms = hdpmodel.get_topics()\n",
    "print(topic_terms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Topic Vectors from Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7235, 150)\n"
     ]
    }
   ],
   "source": [
    "doc_topics = np.zeros((len(corpus), topic_terms.shape[0]))\n",
    "for doc_id in range(len(corpus)):\n",
    "    topic_probs = hdpmodel[corpus[doc_id]]\n",
    "    for topic_id, prob in topic_probs:\n",
    "        doc_topics[doc_id, topic_id] = prob\n",
    "        \n",
    "print(doc_topics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cosine Similarity\n",
    "\n",
    "You can save cycles by parallelizing the operation. So cosine similarity between vectors A and B is defined as:\n",
    "\n",
    "$$cos(\\theta) = \\frac{A \\cdot  B}{\\left \\| A \\right \\| \\left \\| B \\right \\|}$$\n",
    "\n",
    "In order to compute cosine similarity for all pairs pf documents D, each document given by some vector A, we can do the following which is much faster.\n",
    "\n",
    "$$S = \\frac{D * D^T}{\\left \\| D \\right \\|^2}$$\n",
    "\n",
    "For use from within the web tool, we save the similarity matrix to disk so we can use it later without the preceding calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7235, 7235)\n"
     ]
    }
   ],
   "source": [
    "sim = np.matmul(doc_topics, np.transpose(doc_topics)) / np.linalg.norm(doc_topics)\n",
    "print(sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(TOPIC_SIMS, sim)\n",
    "pickle.dump(docid2corpus, open(TOPIC_LOOKUP, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Forward-backward retraining of recurrent neural networks\n",
      "--- top 10 similar docs ---\n",
      "(0.01294) Reinforcement Learning for Call Admission Control and Routing in Integrated Service Networks\n",
      "(0.01293) Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems\n",
      "(0.01292) The Effect of Eligibility Traces on Finding Optimal Memoryless Policies in Partially Observable Markov Decision Processes\n",
      "(0.01292) Learning Macro-Actions in Reinforcement Learning\n",
      "(0.01292) Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System\n",
      "(0.01291) Hippocampal Model of Rat Spatial Abilities Using Temporal Difference Learning\n",
      "(0.01291) Reinforcement Learning for Mixed Open-loop and Closed-loop Control\n",
      "(0.01291) MELONET I: Neural Nets for Inventing Baroque-Style Chorale Variations\n",
      "(0.01289) On-line Policy Improvement using Monte-Carlo Search\n",
      "(0.01289) How to Dynamically Merge Markov Decision Processes\n"
     ]
    }
   ],
   "source": [
    "def similar_docs(filename, sim, topn, docid2corpus, corpusid2doc):\n",
    "    doc_id = int(filename.split(\".\")[0])\n",
    "    corpus_id = docid2corpus[doc_id]\n",
    "    row = sim[corpus_id, :]\n",
    "    target_docs = np.argsort(-row)[0:topn].tolist()\n",
    "    scores = row[target_docs].tolist()\n",
    "    target_filenames = [\"{:d}.txt\".format(corpusid2doc[x]) for x in target_docs]\n",
    "    return target_filenames, scores\n",
    "    \n",
    "\n",
    "filename2title = {}\n",
    "with open(PAPERS_METADATA, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        cols = line.strip().split(\"\\t\")\n",
    "        filename2title[\"{:s}.txt\".format(cols[0])] = cols[2]\n",
    "\n",
    "source_filename = \"1032.txt\"\n",
    "top_n = 10\n",
    "target_filenames, scores = similar_docs(source_filename, sim, top_n, \n",
    "                                        docid2corpus, corpusid2doc)\n",
    "print(\"Source: {:s}\".format(filename2title[source_filename]))\n",
    "print(\"--- top {:d} similar docs ---\".format(top_n))\n",
    "for target_filename, score in zip(target_filenames, scores):\n",
    "    print(\"({:.5f}) {:s}\".format(score, filename2title[target_filename]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
